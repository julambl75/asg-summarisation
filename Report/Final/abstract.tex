\abstract{
\thispagestyle{plain}
To this day, text summarization remains a largely open-ended problem in Natural Language Processing, and is most often resolved using some form of Machine Learning.

In this project, we aim to resolve the problem for short texts about a paragraph in length, via a novel approach that makes use of Answer Set Grammars. By combining ideas from the fields of logic-based learning, knowledge representation and linguistics, we have created a system that is capable of producing partially \textit{abstractive}, \textit{generic} and \textit{informative} summaries, as well as scoring them by order of pertinence and information density.

The approach chosen for this project relies on a central context-free grammar as its internal representation for a simplified version of the English language. Using Answer Set Programming, the system is able to perform logic-based learning to understand the input text after having pre-processed it, the result of which then goes through a number of summarization logic rules, producing a set of possible summaries.

Throughout the development phase, we ran our system on a suite of targeted examples. When the implementation was complete, we successfully trained a neural network to learn the summarization rules used by our framework, thereby validating our approach. We then performed two experiments to evaluate our system, and to establish some of the key differences compared to a neural network.

From this we conclude that although our approach is less general than a neural network, it is able to produce more consistent results and can detect grammatically-incorrect text. Moreover, it does not rely on noisy annotated data, and can be expanded upon without the need to be retrained.
}