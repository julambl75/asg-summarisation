\documentclass[12pt,twoside]{report}
\setcounter{secnumdepth}{3}

\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm,bindingoffset=6mm]{geometry}
%\usepackage{fancyhdr}
\usepackage[pagestyles]{titlesec}
\usepackage{lipsum}
\usepackage{tikz}
\usepackage{float}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{datetime}
\usepackage{subcaption}
\usepackage[titletoc]{appendix}
\usepackage{titlesec}
\usepackage{color,soul} % TODO remove

\usetikzlibrary{shapes,arrows,trees,positioning}
\tikzstyle{block}=[draw, fill=blue!20, minimum size=2em]

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
    citecolor=blue
}

% https://tex.stackexchange.com/questions/81942/div-equivalent-to-mod
\makeatletter
\newcommand*{\bdiv}{%
  \nonscript\mskip-\medmuskip\mkern5mu%
  \mathbin{\operator@font div}\penalty900\mkern5mu%
  \nonscript\mskip-\medmuskip
}
\makeatother

%\pagestyle{fancy}
%\renewcommand{\chaptermark}[1]{\markboth{#1}{#1}}
%\fancyhead{}
%\fancyhead[L]{Chapter \thechapter: \leftmark}
%\fancyhead[R]{\nouppercase{\rightmark}}

\renewcommand*\thesection{\arabic{section}}
\newpagestyle{Headings}{
 \sethead
   {Chapter \thechapter: \chaptertitle}
   {}
   {Section \toptitlemarks\thesection: \toptitlemarks\sectiontitle}
   \headrule
 \setfoot{}{\thepage}{}
}
\newpagestyle{PageNum}{
 \setfoot{}{\thepage}{}
}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{objective}{Objective}

\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}

\graphicspath{ {media/} }

\newcommand{\reporttitle}{Using Answer Set Grammars For Text Summarization}
\newcommand{\reportauthor}{Julien Amblard}
\newcommand{\supervisor}{Alessandra Russo}
\newcommand{\helper}{David Tuckey}
\newcommand{\secondmarker}{Krysia Broda}
\newcommand{\asgauthor}{Mark Law}
\newcommand{\reporttype}{Individual Project}
\newcommand{\degreetype}{Computing MEng}

\begin{document}

\pagestyle{empty}
\input{title}

\pagenumbering{roman}

\tableofcontents

\titleformat{\chapter}
{\normalfont\huge}{\chaptertitlename{} \thechapter}{20pt}{\bfseries\huge}
\titlespacing*{\chapter}{0pt}{0pt}{40pt}

\chapter{Introduction}
\input{chapters/introduction}
\pagestyle{Headings}
\pagenumbering{arabic}

\chapter{Background}
\input{chapters/background}

\chapter{Contributions}
\input{chapters/contributions}

\chapter{Preprocessor}
\input{chapters/preprocessor}

\chapter{ASG}
\input{chapters/asg}

- Learning is not really learning (ASG never learns how to summarize, we build in rules of feature extraction)
- Describe action predicates as high level semantic descriptor of all possible actions that can happen in sentences

- Maybe formalize mathematically task of summarization (with CFG, BK, E+, E-)
    1. CFG is language, BK is leaf nodes, result is actions
    2. CFG is language, BK is leaf nodes, E is actions, result is summaries
- Appendix with summary generation rules

http://universalteacher.org.uk/lang/engstruct.htm

Ideas:
- final fix-up using language\_checker.fix
- hard-code determiners into derivations (https://www.ef.com/wwen/english-resources/english-grammar/determiners/)
- use lots of simple/precise rules rather than complicated/general ones to minimize ss
- keep rules as restricted as possible, when concept implemented over time add missing rules
- to avoid having to add grammar constraints try and rely on grammar of input

- reduce search space using mode bias (simple example: 396->16, very complicated example: 9477->1044)

Choice:
1. Simplify using Python script (faster)
2. Make ASG format more complex (new information probably lost in summary anyway)

- for learning actions do one sentence at a time to minimize ss
- pick best summary according to TTR*

action(INDEX, VERB, SUBJECT, OBJECT)
summary(VERB, SUBJECT, OBJECT)

verb(INDICATIVE\_FORM, TENSE)
subject(NOUN, DET, ADJ\_OR\_ADV)
object(NOUN, DET, ADJ\_OR\_ADV)
noun(NAME)
adj\_or\_adv(NAME)
det(...)
compound(FIRST, SECOND)  for verbs
conjunct(FIRST, SECOND)  learn both

* Type-Token Ratio (TTR): The basic idea behind that measure is that if the text is more complex, the author uses a more varied vocabulary so there’s a larger number of types (unique words). This logic is explicit in the TTR’s formula, which calculates the number of types divided by the number of tokens. As a result, the higher the TTR, the higher the lexical complexity.

\chapter{Post-Processing / Scoring}
\input{chapters/postprocess}

- Fix grammar
- Final goal: take story-specific ASG and general rules to generate summaries, then use top 5/10

\chapter{Evaluation}
\input{chapters/evaluation}

For for each story:
1. Pick predefined lexical field (topic)
2. Pick a single pronoun (p)
3. Pick a single proper noun (pn)
4. For each sentence:
    - Subject: p, pn, or synonym/hyponym/hypernym of topic with optional common adjective for it
    - Verb: verb from same lexical field as topic if possible, otherwise random
    - Object: p, pn, or holonym/meronym of subject with lexical field of currently used common nouns
    

- Compare with NN
    1. Randomize action(...) to generate summary(...) on trained ASG
    2. Train NN to generate same summary(...)
    3. Show framework is sane and expandable (computationally tractable)
    4. Compute Rouge score (PyRouge, must clone repo into project) on ASG and NN

\chapter{Literature Review}
\input{chapters/literature_review}

\begin{appendices}
\titleformat{\chapter}{\normalfont\huge}{\appendixname{} \thechapter.}{20pt}{\bfseries\huge}
\input{appendices/pos}
\input{appendices/asg}
\end{appendices}

%\nocite{*}
\bibliographystyle{vancouver}
\bibliography{references}
\pagestyle{PageNum}

\end{document}