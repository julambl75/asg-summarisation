
@misc{noauthor_examples_nodate,
	title = {Examples of {Short} {Stories}},
	url = {https://examples.yourdictionary.com/examples-of-short-stories.html},
	abstract = {Review examples of short stories and you will probably agree that, whether contemporary or classic, a short story can tell a complete story in a small number of words.},
	language = {en},
	urldate = {2020-01-05},
	journal = {YourDictionary},
	keywords = {stories},
	file = {Snapshot:/Users/julienamblard/Zotero/storage/8GU39Y4H/examples-of-short-stories.html:text/html}
}

@misc{noauthor_short_nodate,
	title = {Short {Stories} for {Kids} {With} {Morals} {\textbar} {Free} {Short} {Stories} for kids {Online}},
	url = {https://www.kidsgen.com/short_stories/},
	urldate = {2020-01-05},
	keywords = {stories},
	file = {Short Stories for Kids With Morals | Free Short Stories for kids Online:/Users/julienamblard/Zotero/storage/CYIQCSHW/short_stories.html:text/html}
}

@misc{noauthor_corenlp_nodate,
	title = {{CoreNLP}},
	url = {https://corenlp.run/},
	urldate = {2020-01-05},
	keywords = {tools},
	file = {:/Users/julienamblard/Zotero/storage/QRGYYY7Y/corenlp.run.html:text/html}
}

@misc{noauthor_spacy_nodate,
	title = {{spaCy} · {Industrial}-strength {Natural} {Language} {Processing} in {Python}},
	url = {https://spacy.io/},
	abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
	language = {en},
	urldate = {2020-01-05},
	keywords = {tools},
	file = {Snapshot:/Users/julienamblard/Zotero/storage/3GA8Q38Y/spacy.io.html:text/html}
}

@article{law_inductive_nodate,
	title = {Inductive {Learning} of {Answer} {Set} {Programs} - {User} {Manual}},
	abstract = {ILASP[8] (Inductive Learning of Answer Set Programs) is a system for learning ASP (Answer Set Programming) programs from examples. It is capable of learning normal rules, choice rules and constraints through positive and negative examples of partial answer sets. ILASP can also learn preferences, represented as weak constraints in ASP by considering example ordered pairs of partial answer sets.},
	language = {en},
	author = {Law, M and Russo, A and Broda, K},
	pages = {25},
	file = {Law et al. - Inductive Learning of Answer Set Programs - User M.pdf:/Users/julienamblard/Zotero/storage/5LKRG7HC/Law et al. - Inductive Learning of Answer Set Programs - User M.pdf:application/pdf}
}

@article{law_fastlas_nodate,
	title = {{FastLAS}: {Scalable} {Inductive} {Logic} {Programming} incorporating {Domain}-specific {Optimisation} {Criteria}},
	abstract = {Inductive Logic Programming (ILP) systems aim to ﬁnd a set of logical rules, called a hypothesis, that explain a set of examples. In cases where many such hypotheses exist, ILP systems often bias towards shorter solutions, leading to highly general rules being learned. In some application domains like security and access control policies, this bias may not be desirable, as when data is sparse more speciﬁc rules that guarantee tighter security should be preferred. This paper presents a new general notion of a scoring function over hypotheses that allows a user to express domain-speciﬁc optimisation criteria. This is incorporated into a new ILP system, called FastLAS, that takes as input a learning task and a customised scoring function, and computes an optimal solution with respect to the given scoring function. We evaluate the accuracy of FastLAS over real-world datasets for access control policies and show that varying the scoring function allows a user to target domain-speciﬁc performance metrics. We also compare FastLAS to state-of-the-art ILP systems, using the standard ILP bias for shorter solutions, and demonstrate that FastLAS is signiﬁcantly faster and more scalable.},
	language = {en},
	author = {Law, Mark and Russo, Alessandra and Bertino, Elisa and Broda, Krysia and Lobo, Jorge},
	keywords = {tools},
	pages = {9},
	file = {Law et al. - FastLAS Scalable Inductive Logic Programming inco.pdf:/Users/julienamblard/Zotero/storage/5DRSKP2M/Law et al. - FastLAS Scalable Inductive Logic Programming inco.pdf:application/pdf}
}

@misc{noauthor_syntactic_nodate,
	title = {Syntactic {Parsing} with {CoreNLP} and {NLTK} {\textbar} {District} {Data} {Labs}},
	url = {https://www.districtdatalabs.com/syntax-parsing-with-corenlp-and-nltk},
	abstract = {Syntactic parsing analyzes text for its underlying data. Learn to master this difficult task with the best parsing tool, Stanford's CoreNLP Library.},
	language = {en-US},
	urldate = {2020-01-05},
	journal = {District Data Labs: Data Science Consulting and Training},
	file = {Snapshot:/Users/julienamblard/Zotero/storage/9BHRIWF9/syntax-parsing-with-corenlp-and-nltk.html:text/html}
}

@article{gomez-rodriguez_how_2019,
	title = {How important is syntactic parsing accuracy? {An} empirical evaluation on rule-based sentiment analysis},
	volume = {52},
	issn = {0269-2821},
	shorttitle = {How important is syntactic parsing accuracy?},
	doi = {10.1007/s10462-017-9584-0},
	abstract = {Syntactic parsing, the process of obtaining the internal structure of sentences in natural languages, is a crucial task for artificial intelligence applications that need to extract meaning from natural language text or speech. Sentiment analysis is one example of application for which parsing has recently proven useful. In recent years, there have been significant advances in the accuracy of parsing algorithms. In this article, we perform an empirical, task-oriented evaluation to determine how parsing accuracy influences the performance of a state-of-the-art rule-based sentiment analysis system that determines the polarity of sentences from their parse trees. In particular, we evaluate the system using four well-known dependency parsers, including both current models with state-of-the-art accuracy and more innacurate models which, however, require less computational resources. The experiments show that all of the parsers produce similarly good results in the sentiment analysis task, without their accuracy having any relevant influence on the results. Since parsing is currently a task with a relatively high computational cost that varies strongly between algorithms, this suggests that sentiment analysis researchers and users should prioritize speed over accuracy when choosing a parser; and parsing researchers should investigate models that improve speed further, even at some cost to accuracy.},
	language = {English},
	number = {3},
	journal = {Artificial Intelligence Review},
	author = {Gomez-Rodriguez, Carlos and Alonso-Alonso, Iago and Vilares, David},
	month = oct,
	year = {2019},
	note = {WOS:000486256400018},
	keywords = {Artificial intelligence, Natural language processing, Sentiment analysis, Syntactic parsing},
	pages = {2081--2097},
	file = {Submitted Version:/Users/julienamblard/Zotero/storage/RVP2LNFM/Gomez-Rodriguez et al. - 2019 - How important is syntactic parsing accuracy An em.pdf:application/pdf}
}

@article{law_representing_2019,
	title = {Representing and {Learning} {Grammars} in {Answer} {Set} {Programming}},
	volume = {33},
	issn = {2374-3468, 2159-5399},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/4147},
	doi = {10.1609/aaai.v33i01.33012919},
	abstract = {In this paper we introduce an extension of context-free grammars called answer set grammars (ASGs). These grammars allow annotations on production rules, written in the language of Answer Set Programming (ASP), which can express context-sensitive constraints. We investigate the complexity of various classes of ASG with respect to two decision problems: deciding whether a given string belongs to the language of an ASG and deciding whether the language of an ASG is non-empty. Speciﬁcally, we show that the complexity of these decision problems can be lowered by restricting the subset of the ASP language used in the annotations. To aid the applicability of these grammars to computational problems that require context-sensitive parsers for partially known languages, we propose a learning task for inducing the annotations of an ASG. We characterise the complexity of this task and present an algorithm for solving it. An evaluation of a (prototype) implementation is also discussed.},
	language = {en},
	urldate = {2020-01-05},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Law, Mark and Russo, Alessandra and Bertino, Elisa and Broda, Krysia and Lobo, Jorge},
	month = jul,
	year = {2019},
	pages = {2919--2928},
	file = {Law et al. - 2019 - Representing and Learning Grammars in Answer Set P.pdf:/Users/julienamblard/Zotero/storage/XAIK864J/Law et al. - 2019 - Representing and Learning Grammars in Answer Set P.pdf:application/pdf;4147-Article Text-7201-1-10-20190705.pdf:/Users/julienamblard/Zotero/storage/XXL7FAXD/4147-Article Text-7201-1-10-20190705.pdf:application/pdf}
}

@article{azhari_improving_2017,
	title = {Improving text summarization using neuro-fuzzy approach},
	volume = {1},
	issn = {2475-1839, 2475-1847},
	url = {https://www.tandfonline.com/doi/full/10.1080/24751839.2017.1364040},
	doi = {10.1080/24751839.2017.1364040},
	abstract = {In today’s digital era, it becomes a challenge for netizens to find specific information on the internet. Many web-based documents are retrieved and it is not easy to digest all the retrieved information. Automatic text summarization is a process that identifies the important points from all the related documents to produce a concise summary. In this paper, we propose a text summarization model based on classification using neuro-fuzzy approach. The model can be trained to filter high-quality summary sentences. We then compare the performance of our proposed model with the existing approaches, which are based on fuzzy logic and neural network techniques. ANFIS showed improved results compared to the previous techniques in terms of average precision, recall and F-measure on the Document Understanding Conference (DUC) data corpus.},
	language = {en},
	number = {4},
	urldate = {2020-01-05},
	journal = {Journal of Information and Telecommunication},
	author = {Azhari, Muhammad and Jaya Kumar, Yogan},
	month = oct,
	year = {2017},
	keywords = {literature},
	pages = {367--379},
	file = {Azhari and Jaya Kumar - 2017 - Improving text summarization using neuro-fuzzy app.pdf:/Users/julienamblard/Zotero/storage/QK8EFUEF/Azhari and Jaya Kumar - 2017 - Improving text summarization using neuro-fuzzy app.pdf:application/pdf}
}

@article{yao_dual_2018,
	title = {Dual {Encoding} for {Abstractive} {Text} {Summarization}},
	issn = {2168-2275},
	doi = {10.1109/TCYB.2018.2876317},
	abstract = {Recurrent neural network-based sequence-to-sequence attentional models have proven effective in abstractive text summarization. In this paper, we model abstractive text summarization using a dual encoding model. Different from the previous works only using a single encoder, the proposed method employs a dual encoder including the primary and the secondary encoders. Specifically, the primary encoder conducts coarse encoding in a regular way, while the secondary encoder models the importance of words and generates more fine encoding based on the input raw text and the previously generated output text summarization. The two level encodings are combined and fed into the decoder to generate more diverse summary that can decrease repetition phenomenon for long sequence generation. The experimental results on two challenging datasets (i.e., CNN/DailyMail and DUC 2004) demonstrate that our dual encoding model performs against existing methods.},
	journal = {IEEE Transactions on Cybernetics},
	author = {Yao, Kaichun and Zhang, Libo and Du, Dawei and Luo, Tiejian and Tao, Lili and Wu, Yanjun},
	year = {2018},
	keywords = {Abstractive text summarization, Computational modeling, Decoding, dual encoding, Encoding, primary encoder, recurrent neural network (RNN), Recurrent neural networks, secondary encoder, Semantics, Task analysis, literature},
	pages = {1--12},
	file = {IEEE Xplore Full Text PDF:/Users/julienamblard/Zotero/storage/P7FZ6RHW/Yao et al. - 2018 - Dual Encoding for Abstractive Text Summarization.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/julienamblard/Zotero/storage/PS8NIDZW/8520903.html:text/html}
}

@article{lynn_improved_2018,
	title = {An improved method of automatic text summarization for web contents using lexical chain with semantic-related terms},
	volume = {22},
	issn = {1432-7643, 1433-7479},
	url = {http://link.springer.com/10.1007/s00500-017-2612-9},
	doi = {10.1007/s00500-017-2612-9},
	abstract = {Many researches have been converging on automatic text summarization as increasing of text documents due to the expansion of information diffusion constantly. The objective of this proposal is to achieve the most reliable and substantial context or most relevant brief summary of the text in extractive manner. The extractive text summarization produces the short summary of a certain text which contains the most important information of original text by extracting the set of sentences from the original document. This paper proposes an improved extractive text summarization method for documents by enhancing the conventional lexical chain method to produce better relevant information of the text using three distinct features or characteristics of keyword in a text. The keyword of the document is labeled using our previous work, transition probability distribution generator model which can learn the characteristics of the keyword in a document, and generates their probability distribution upon each feature.},
	language = {en},
	number = {12},
	urldate = {2020-01-05},
	journal = {Soft Computing},
	author = {Lynn, Htet Myet and Choi, Chang and Kim, Pankoo},
	month = jun,
	year = {2018},
	keywords = {literature},
	pages = {4013--4023},
	file = {Lynn et al. - 2018 - An improved method of automatic text summarization.pdf:/Users/julienamblard/Zotero/storage/5KRKSLVG/Lynn et al. - 2018 - An improved method of automatic text summarization.pdf:application/pdf}
}

@article{kiyani_survey_2017,
	title = {A survey automatic text summarization},
	volume = {5},
	issn = {2146-7943},
	url = {http://www.pressacademia.org/images/documents/procedia/archives/vol_5/029.pdf},
	doi = {10.17261/Pressacademia.2017.591},
	abstract = {Text summarization is compress the source text into a diminished version conserving its information content and overall meaning. Because of the great amount of the information we are provided it and thanks to development of Internet Technologies, text summarization has become an important tool for interpreting text information. Text summarization methods can be classified into extractive and abstractive summarization. An extractive summarization method involves selecting sentences of high rank from the document based on word and sentence features and put them together to generate summary. The importance of the sentences is decided based on statistical and linguistic features of sentences. An abstractive summarization is used to understanding the main concepts in a given document and then expresses those concepts in clear natural language. In this paper, gives comparative study of various text summarization techniques.},
	language = {en},
	number = {1},
	urldate = {2020-01-06},
	journal = {Pressacademia},
	author = {Kiyani, Farzad and Tas, Oguzhan},
	month = jun,
	year = {2017},
	keywords = {literature},
	pages = {205--213},
	file = {Kiyani and Tas - 2017 - A survey automatic text summarization.pdf:/Users/julienamblard/Zotero/storage/SPRC5AT6/Kiyani and Tas - 2017 - A survey automatic text summarization.pdf:application/pdf}
}

@article{lehnert_1980_nodate,
	title = {1980 - {Narrative} {Text} {Summarization}},
	abstract = {In order to summarize a story it is necessary to access a high level analysis that highlights the story's central concepts. A technique of memory representation based on affect units appears to provide the necessary foundation for such an analysis. Affect units are conceptual structures that overlap with each other when a narrative is cohesive. When overlapping intersections are interpreted as arcs in a graph of affect units, the resulting graph encodes the plot of the story. Structural features of the graph then reveal which concepts are central to the story. Affect unit analysis is currently being investigated as a processing strategy for narrative summarization.},
	language = {en},
	author = {Lehnert, Wendy G},
	keywords = {literature},
	pages = {3},
	file = {Lehnert - 1980 - Narrative Text Summarization.pdf:/Users/julienamblard/Zotero/storage/ZF2FB8NV/Lehnert - 1980 - Narrative Text Summarization.pdf:application/pdf}
}

@article{clark_combining_nodate,
	title = {Combining {Symbolic} and {Distributional} {Models} of {Meaning}},
	abstract = {The are two main approaches to the representation of meaning in Computational Linguistics: a symbolic approach and a distributional approach. This paper considers the fundamental question of how these approaches might be combined. The proposal is to adapt a method from the Cognitive Science literature, in which symbolic and connectionist representations are combined using tensor products. Possible applications of this method for language processing are described. Finally, a potentially fruitful link between Quantum Mechanics, Computational Linguistics, and other related areas such as Information Retrieval and Machine Learning, is proposed.},
	language = {en},
	author = {Clark, Stephen},
	keywords = {literature},
	pages = {4},
	file = {Clark - Combining Symbolic and Distributional Models of Me.pdf:/Users/julienamblard/Zotero/storage/YNUHYD45/Clark - Combining Symbolic and Distributional Models of Me.pdf:application/pdf}
}

@misc{noauthor_studying_nodate,
	title = {Studying {Ambiguous} {Sentences}},
	url = {https://www.byrdseed.com/ambiguous-sentences/},
	abstract = {This type of sentence has great possibilities for classroom application because of its two different interpretations. It's a perfect tool to: demonstrate careful reading, showcase the need for editing while writing, and encourage creativity and divergent thinking.},
	language = {en-US},
	urldate = {2020-01-06},
	journal = {Byrdseed.com},
	keywords = {literature},
	file = {Snapshot:/Users/julienamblard/Zotero/storage/FEGVF86F/ambiguous-sentences.html:text/html}
}

@incollection{hutchison_inductive_2012,
	address = {Berlin, Heidelberg},
	title = {Inductive {Logic} {Programming} in {Answer} {Set} {Programming}},
	volume = {7207},
	isbn = {978-3-642-31950-1 978-3-642-31951-8},
	url = {http://link.springer.com/10.1007/978-3-642-31951-8_12},
	abstract = {In this paper we discuss the design of an Inductive Logic Programming system in Answer Set Programming and more in general the problem of integrating the two. We show how to formalise the learning problem as an ASP program and provide details on how the optimisation features of modern solvers can be adapted to derive preferred hypotheses.},
	language = {en},
	urldate = {2020-01-06},
	booktitle = {Inductive {Logic} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Corapi, Domenico and Russo, Alessandra and Lupu, Emil},
	editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Muggleton, Stephen H. and Tamaddoni-Nezhad, Alireza and Lisi, Francesca A.},
	year = {2012},
	doi = {10.1007/978-3-642-31951-8_12},
	pages = {91--97},
	file = {Corapi et al. - 2012 - Inductive Logic Programming in Answer Set Programm.pdf:/Users/julienamblard/Zotero/storage/JMT65ELQ/Corapi et al. - 2012 - Inductive Logic Programming in Answer Set Programm.pdf:application/pdf}
}

@article{lifschitz_what_nodate,
	title = {What {Is} {Answer} {Set} {Programming}?},
	abstract = {Answer set programming (ASP) is a form of declarative programming oriented towards difﬁcult search problems. As an outgrowth of research on the use of nonmonotonic reasoning in knowledge representation, it is particularly useful in knowledge-intensive applications. ASP programs consist of rules that look like Prolog rules, but the computational mechanisms used in ASP are different: they are based on the ideas that have led to the creation of fast satisﬁability solvers for propositional logic.},
	language = {en},
	author = {Lifschitz, Vladimir},
	keywords = {literature},
	pages = {4},
	file = {Lifschitz - What Is Answer Set Programming.pdf:/Users/julienamblard/Zotero/storage/J3E4T24G/Lifschitz - What Is Answer Set Programming.pdf:application/pdf}
}

@article{yeh_text_2005,
	title = {Text summarization using a trainable summarizer and latent semantic analysis},
	volume = {41},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457304000329},
	doi = {10.1016/j.ipm.2004.04.003},
	abstract = {This paper proposes two approaches to address text summarization: modiﬁed corpus-based approach (MCBA) and LSA-based T.R.M. approach (LSA + T.R.M.). The ﬁrst is a trainable summarizer, which takes into account several features, including position, positive keyword, negative keyword, centrality, and the resemblance to the title, to generate summaries. Two new ideas are exploited: (1) sentence positions are ranked to emphasize the signiﬁcances of diﬀerent sentence positions, and (2) the score function is trained by the genetic algorithm (GA) to obtain a suitable combination of feature weights. The second uses latent semantic analysis (LSA) to derive the semantic matrix of a document or a corpus and uses semantic sentence representation to construct a semantic text relationship map. We evaluate LSA + T.R.M. both with single documents and at the corpus level to investigate the competence of LSA in text summarization. The two novel approaches were measured at several compression rates on a data corpus composed of 100 political articles. When the compression rate was 30\%, an average f-measure of 49\% for MCBA, 52\% for MCBA + GA, 44\% and 40\% for LSA + T.R.M. in single-document and corpus level were achieved respectively.},
	language = {en},
	number = {1},
	urldate = {2020-01-07},
	journal = {Information Processing \& Management},
	author = {Yeh, Jen-Yuan and Ke, Hao-Ren and Yang, Wei-Pang and Meng, I-Heng},
	month = jan,
	year = {2005},
	keywords = {literature},
	pages = {75--95},
	file = {Yeh et al. - 2005 - Text summarization using a trainable summarizer an.pdf:/Users/julienamblard/Zotero/storage/7VY3RFJE/Yeh et al. - 2005 - Text summarization using a trainable summarizer an.pdf:application/pdf}
}

@article{powers_evaluation_2011,
	title = {Evaluation: from {Precision}, {Recall} and {F}-measure to {ROC}, {Informedness}, {Markedness} and {Correlation}},
	copyright = {Author retains copyright of this version.},
	issn = {2229-3981},
	shorttitle = {Evaluation},
	url = {https://dspace.flinders.edu.au/xmlui/handle/2328/27165},
	abstract = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are
biased and should not be used without clear understanding of the biases, and corresponding identification of chance
or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of
Informedness, can appear to perform better under any of these commonly used measures. We discuss several
concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and
introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we
demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance
as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case
to the general multi-class case.},
	language = {en},
	urldate = {2020-01-07},
	author = {Powers, David Martin},
	month = dec,
	year = {2011},
	keywords = {literature},
	file = {Full Text PDF:/Users/julienamblard/Zotero/storage/P8MHLJRJ/Powers - 2011 - Evaluation from Precision, Recall and F-measure t.pdf:application/pdf;Snapshot:/Users/julienamblard/Zotero/storage/IHSSJTBY/27165.html:text/html}
}

@article{barzilay_using_1997,
	title = {Using lexical chains for text summarization},
	url = {https://doi.org/10.7916/D85B09VZ},
	doi = {10.7916/D85B09VZ},
	abstract = {We investigate one technique to produce a summary of an original text without requiring its full semantic interpretation, but instead relying on a model of the topic progression in the text derived from lexical chains. We present a new algorithm to compute lexical chains in a text, merging several robust knowledge sources: the WordNet thesaurus, a part-of-speech tagger, shallow parser for the identification of nominal groups, and a segmentation algorithm. Summarization proceeds in four steps: the original text is segmented, lexical chains are constructed, strong chains are identified and significant sentences are extracted. We present in this paper empirical results on the identification of strong chains and of significant sentences. Preliminary results indicate that quality indicative summaries are produced. Pending problems are identified. Plans to address these short-comings are briefly presented.},
	language = {en},
	urldate = {2020-01-07},
	author = {Barzilay, Regina and Elhadad, Michael},
	year = {1997},
	keywords = {literature},
	file = {Full Text PDF:/Users/julienamblard/Zotero/storage/X5N2LKY3/Barzilay and Elhadad - 1997 - Using lexical chains for text summarization.pdf:application/pdf;Snapshot:/Users/julienamblard/Zotero/storage/UTDBVFP4/D85B09VZ.html:text/html}
}

@article{scheinberg_note_1960,
	title = {Note on the boolean properties of context free languages},
	volume = {3},
	issn = {00199958},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0019995860909657},
	doi = {10.1016/S0019-9958(60)90965-7},
	language = {en},
	number = {4},
	urldate = {2020-01-08},
	journal = {Information and Control},
	author = {Scheinberg, Stephen},
	month = dec,
	year = {1960},
	keywords = {literature},
	pages = {372--375},
	file = {Scheinberg - 1960 - Note on the boolean properties of context free lan.pdf:/Users/julienamblard/Zotero/storage/E2QLJY8P/Scheinberg - 1960 - Note on the boolean properties of context free lan.pdf:application/pdf}
}

@article{steedman_combinatory_nodate,
	title = {Combinatory {Categorial} {Grammar}},
	abstract = {Combinatory Categorial Grammar (CCG) is a radically lexicalized theory of grammar in which all language-speciﬁc information, including the linear order of heads, arguments, and adjuncts, is speciﬁed in the lexicon, from which it is projected onto sentences by language-independent universal typedependent combinatory rules of low “slightly non-context-free” expressive power, applying to strictly adjacent phonologically-realised categories. Syntactic and phonological derivation are isomorphic, and are synchronously coupled with semantic composition in a purely type-dependent rule-to-rule relation.},
	language = {en},
	author = {Steedman, Mark},
	keywords = {literature},
	pages = {31},
	file = {Steedman - Combinatory Categorial Grammar.pdf:/Users/julienamblard/Zotero/storage/VZZTA5EW/Steedman - Combinatory Categorial Grammar.pdf:application/pdf}
}

@inproceedings{kowalski_predicate_1974,
	title = {Predicate logic as programming language},
	volume = {74},
	booktitle = {{IFIP} congress},
	author = {Kowalski, Robert},
	year = {1974},
	pages = {569--544},
	file = {Kowalski - 1974 - Predicate logic as programming language.pdf:/Users/julienamblard/Zotero/storage/NPQD7M6C/Kowalski - 1974 - Predicate logic as programming language.pdf:application/pdf}
}

@article{knight_statistics-based_2000,
	title = {Statistics-based summarization-step one: {Sentence} compression},
	volume = {2000},
	journal = {AAAI/IAAI},
	author = {Knight, Kevin and Marcu, Daniel},
	year = {2000},
	keywords = {literature},
	pages = {703--710},
	file = {Knight and Marcu - 2000 - Statistics-based summarization-step one Sentence .pdf:/Users/julienamblard/Zotero/storage/8VWJMZYW/Knight and Marcu - 2000 - Statistics-based summarization-step one Sentence .pdf:application/pdf}
}

@article{lloret_text_2008,
	title = {Text summarization: an overview},
	journal = {Paper supported by the Spanish Government under the project TEXT-MESS (TIN2006-15265-C06-01)},
	author = {Lloret, Elena},
	year = {2008},
	keywords = {literature}
}

@misc{noauthor_penn_nodate,
	title = {Penn {Treebank} {P}.{O}.{S}. {Tags}},
	url = {https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html},
	urldate = {2020-01-13},
	keywords = {appendix},
	file = {Penn Treebank P.O.S. Tags:/Users/julienamblard/Zotero/storage/8YQ9ID72/penn_treebank_pos.html:text/html}
}

@article{radev_introduction_2002,
	title = {Introduction to the {Special} {Issue} on {Summarization}},
	volume = {28},
	issn = {0891-2017, 1530-9312},
	url = {http://www.mitpressjournals.org/doi/10.1162/089120102762671927},
	doi = {10.1162/089120102762671927},
	language = {en},
	number = {4},
	urldate = {2020-01-17},
	journal = {Computational Linguistics},
	author = {Radev, Dragomir R. and Hovy, Eduard and McKeown, Kathleen},
	month = dec,
	year = {2002},
	pages = {399--408},
	file = {Radev et al. - 2002 - Introduction to the Special Issue on Summarization.pdf:/Users/julienamblard/Zotero/storage/725N345U/Radev et al. - 2002 - Introduction to the Special Issue on Summarization.pdf:application/pdf}
}

@article{partee_lecture_nodate,
	title = {Lecture 2. {Lambda} abstraction, {NP} semantics, and a {Fragment} of {English}},
	language = {en},
	journal = {Formal Semantics},
	author = {Partee, B},
	keywords = {literature},
	pages = {11},
	file = {Partee - Lecture 2. Lambda abstraction, NP semantics, and a.pdf:/Users/julienamblard/Zotero/storage/K3BGWVZL/Partee - Lecture 2. Lambda abstraction, NP semantics, and a.pdf:application/pdf}
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2020-05-30},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {background, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: EMNLP 2014},
	file = {arXiv Fulltext PDF:/Users/julienamblard/Zotero/storage/YCJQAJ32/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf;arXiv.org Snapshot:/Users/julienamblard/Zotero/storage/6ZZWFCKT/1406.html:text/html}
}

@article{luong_effective_2015,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1508.04025},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	language = {en},
	urldate = {2020-05-30},
	journal = {arXiv:1508.04025 [cs]},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	note = {arXiv: 1508.04025},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details},
	file = {Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:/Users/julienamblard/Zotero/storage/DPFX5YE6/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf}
}

@article{gers_learning_2000,
	title = {Learning to {Forget}: {Continual} {Prediction} with {LSTM}},
	volume = {12},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Learning to {Forget}},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976600300015015},
	doi = {10.1162/089976600300015015},
	abstract = {Long Short-Term Memory (LSTM, Hochreiter \& Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow inde nitely and eventually cause the network to break down. Our remedy is a novel, adaptive {\textbackslash}forget gate" that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them in an elegant way.},
	language = {en},
	number = {10},
	urldate = {2020-05-30},
	journal = {Neural Computation},
	author = {Gers, Felix A. and Schmidhuber, Jürgen and Cummins, Fred},
	month = oct,
	year = {2000},
	keywords = {background},
	pages = {2451--2471},
	file = {Gers et al. - 2000 - Learning to Forget Continual Prediction with LSTM.pdf:/Users/julienamblard/Zotero/storage/BKV5WACR/Gers et al. - 2000 - Learning to Forget Continual Prediction with LSTM.pdf:application/pdf}
}

@inproceedings{graves_hybrid_2013,
	address = {Olomouc, Czech Republic},
	title = {Hybrid speech recognition with {Deep} {Bidirectional} {LSTM}},
	isbn = {978-1-4799-2756-2},
	url = {http://ieeexplore.ieee.org/document/6707742/},
	doi = {10.1109/ASRU.2013.6707742},
	abstract = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-speciﬁc objective functions, which are difﬁcult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We ﬁnd that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates.},
	language = {en},
	urldate = {2020-05-30},
	booktitle = {2013 {IEEE} {Workshop} on {Automatic} {Speech} {Recognition} and {Understanding}},
	publisher = {IEEE},
	author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
	month = dec,
	year = {2013},
	keywords = {background},
	pages = {273--278},
	file = {Graves et al. - 2013 - Hybrid speech recognition with Deep Bidirectional .pdf:/Users/julienamblard/Zotero/storage/7ILDPYT2/Graves et al. - 2013 - Hybrid speech recognition with Deep Bidirectional .pdf:application/pdf}
}

@article{luong_effective_2015-1,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1508.04025},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	language = {en},
	urldate = {2020-05-30},
	journal = {arXiv:1508.04025 [cs]},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	month = sep,
	year = {2015},
	note = {arXiv: 1508.04025},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details},
	file = {Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:/Users/julienamblard/Zotero/storage/UXZQKNWB/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf}
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2020-05-30},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {background, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {arXiv Fulltext PDF:/Users/julienamblard/Zotero/storage/EYV3C6NU/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/julienamblard/Zotero/storage/IZ9TUIFH/1409.html:text/html}
}

@article{kubler_dependency_nodate,
	title = {Dependency {Parsing}},
	language = {en},
	author = {Kübler, Sandra},
	keywords = {background},
	pages = {40},
	file = {Kübler - Dependency Parsing.pdf:/Users/julienamblard/Zotero/storage/IT4EIHDP/Kübler - Dependency Parsing.pdf:application/pdf}
}

@article{apt_logic_1990,
	title = {Logic {Programming}.},
	volume = {1990},
	journal = {Handbook of Theoretical Computer Science, Volume B: Formal Models and Sematics (B)},
	author = {Apt, Krzysztof R},
	year = {1990},
	keywords = {background},
	pages = {493--574},
	file = {Apt - 1990 - Logic Programming..pdf:/Users/julienamblard/Zotero/storage/9D2BZ4V3/Apt - 1990 - Logic Programming..pdf:application/pdf}
}

@book{krose_introduction_1993,
	title = {An introduction to {Neural} {Networks}},
	abstract = {Contents  Preface 9 I FUNDAMENTALS 11 1 Introduction 13 2 Fundamentals 15 2.1 A framework for distributed representation : : : : : : : : : : : : : : : : : : : : : 15 2.1.1 Processing units : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 15 2.1.2 Connections between units : : : : : : : : : : : : : : : : : : : : : : : : : : 16 2.1.3 Activation and output rules : : : : : : : : : : : : : : : : : : : : : : : : : : 16 2.2 Network topologies : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 17 2.3 Training of artificial neural networks : : : : : : : : : : : : : : : : : : : : : : : : : 18 2.3.1 Paradigms of learning : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 18 2.3.2 Modifying patterns of connectivity : : : : : : : : : : : : : : : : : : : : : : 18 2.4 Notation and terminology : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 18 2.4.1 Notation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 19 2.4.2 Termino},
	author = {Kröse, Ben and Krose, Ben and Smagt, Patrick van der and Smagt, Patrick},
	year = {1993},
	keywords = {background},
	file = {Citeseer - Snapshot:/Users/julienamblard/Zotero/storage/YDUGE52N/summary.html:text/html;Citeseer - Full Text PDF:/Users/julienamblard/Zotero/storage/U64QSEY6/Kröse et al. - 1993 - An introduction to Neural Networks.pdf:application/pdf}
}
