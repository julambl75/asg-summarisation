\label{chapter:introduction}

In general, the task of summarization in Natural Language Processing (NLP) is to produce a shortened text which covers the main points expressed in a longer text given as input. To this end, a system performing such a task must analyse and process the input in order to extract from it the most important information.

\section{Motivations}

In recent years, state-of-the-art systems that accomplish text summarization have relied largely on Machine Learning. These include Bayesian classifiers, hidden Markov models, neural networks and fuzzy logic, among others \cite{kiyani_survey_2017}. Given a training corpus, along with some careful preprocessing as well as fine-tuning of hyper-parameters and feature extraction functions, such systems are able to produce effective summaries. 

However, to learn what is a summary, these systems require a very large amount of data, and take a long time to train. In contrast, using logic means that we can hard-code this definition directly into our program, avoiding the problem of randomness and uncertainty. By carefully constructing its structure, we can get results with just a short list of rules, and know that it will always produce a complete and valid output with respect to the background knowledge we encode into it.

\section{Objectives}

The main goal of this project is to explore the task of text summarization via logic-based learning with Answer Set Grammars (ASG). Below you will find the principal objectives which were established as being vital to achieving this goal.

\begin{objective}[Research Existing Summarization Methods]
Before diving into the task of logic-based summary generation, we should investigate the approaches used by existing state-of-the-art systems. Even if these are not logic-based, they might rely on techniques that could be beneficial to use for this project.
\end{objective}

\begin{objective}[Translate English Into ASG]
Our system should be capable of taking a text written in English and converting it into some logic-based form that can be interpreted by ASG. Moreover, this internal representation should capture as much of the semantics from the original text as possible, and not be limited to a particular domain.
\end{objective}

\begin{objective}[Generate Summaries Automatically]
Given a brief paragraph of text, for example a short story aimed at young children, we should be able to provide a grammatically correct summary in multiple sentences. This should be fully-automated and not require any human intervention during the process.
\end{objective}

\begin{objective}[Evaluate The Approach]
Once we have implemented the basic approach, we should run our system on a suite of examples to verify that it can produce summaries that closely resemble the corresponding human-generated ones. On a larger scale, it is important to also run it against a popular summarization approach to ensure the sanity of our logic-based mechanism.
\end{objective}

\section{Approach Overview}

In what follows, we shall give a brief overview of the various tasks that were undertaken and completed as part of the project. Where relevant, references to the corresponding sections are provided.

The approach described in this paper, known as \textsc{SumASG*}, can be diagrammatically represented as a three step pipeline, as seen in Figure \ref{fig:main_pipeline}. Although the focus of this project is a mechanism written in ASG, it relies on a number of Python scripts to process information and coordinate its flow.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=0.3cm, auto]
\node (story) [] {Story};
\node (preprocessor) [block, right =of story] {Preprocessor};
\node (asg) [block, right =of preprocessor] {SumASG};
\node (score) [block, right =of asg] {Post-Processing/Scoring};
\node (summaries) [right =of score] {Scored Summaries};
\draw [->] (story) -- (preprocessor);
\draw [->] (preprocessor) -- (asg);
\draw [->] (asg) -- (score);
\draw [->] (score) -- (summaries);
\end{tikzpicture}
\caption{Main Pipeline}
\label{fig:main_pipeline}
\end{figure}

We begin by describing the essential role of the \textsc{Preprocessor} in Chapter \ref{chapter:preprocessor}. Given an input story, its goal is to simplify the story's sentences into a simpler and more consistent structure that will then be easier to parse by ASG (Section \ref{sec:tokenization_scoring}). On top of this, the \textsc{Preprocessor} also removes irrelevant sentences from the story and reduces the lexical diversity (Section \ref{sec:pruning_homogenization}), which helps increase the chances of generating a high-quality summary.

In Chapter \ref{chapter:asg} we discuss the use of ASG in the context of this project, forming a procedure we call \textsc{SumASG}. It all revolves around a purpose-built internal representation of English sentences, represented as a tree (Section \ref{sec:internal_representation}). The first of two steps, \textsc{SumASG\textsubscript{1}}, involves translating sentences from the input story into our internal representation using ASG's learning abilities (Section \ref{sec:learn_actions}). From this, we then use a number of logic-based rules to generate sentences which may be used to form a summary (Section \ref{sec:gen_summary_sentences}).

The third part of the pipeline, as outlined in Chapter \ref{chapter:postprocessing}, serves to turn the output of \textsc{SumASG} into usable summaries. To begin, we post-process the summary sentences given to us as output, and combine them in different ways so as to form potential summaries (Section \ref{sec:summary_creation}). Afterwards, we discuss a mechanism used to score these summaries (Section \ref{sec:scoring}), and explore ways in which to select those that are optimal (Section \ref{sec:summary_selection}).

\subsection{Example}

Throughout this paper, we shall be using the example of the story of Peter Little to illustrate the different steps of our pipeline, as shown below in Figure \ref{fig:peter_little}.

Additional examples of stories can be found in Appendix \ref{appendix:stories}, along with summaries generated by \textsc{SumASG*}.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
There was a curious little boy named Peter Little. He was interested in stars and planets. So he was serious in school and always did his homework. When he was older, he studied mathematics and quantum physics. He studied hard for his exams and became an astrophysicist. Now he is famous.
\end{displayquote}
\caption{Story of Peter Little}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{A.} Peter Little was interested in space so he studied hard and became a famous astrophysicist.\\
\textbf{B.} Peter Little was curious about astronomy. He was always serious in school, and now he is famous.
\end{displayquote}
\caption{\textit{Reference summaries}}
\end{subfigure}
\caption{Example of the task of summarization for the story of Peter Little}
\label{fig:peter_little}
\end{figure}

\section{Contributions}

The main contribution of this project to the field of NLP is the creation of an end-to-end, fully-automated logic-based system capable of text summarization, without the need of any training whatsoever, as would be the case with a typical Machine Learning-based approach these days. Going more into depth, we shall discuss some specific contributions in what follows.

\begin{contribution}[Simplification Of Complex Structures]
Implemented an algorithm that dramatically reduces the complexity in the structure of English sentences, without losing too much information.
\end{contribution}

\begin{contribution}[Removal Of Irrelevant Sentences And Homogenization]
Implemented an algorithm which uses \textit{similarity} to remove irrelevant sentences from a short story, and replaces synonyms with a single representative in each set of synonyms.
\end{contribution}

\begin{contribution}[Representation Of English In ASG]
Created a context-free grammar that models the structure of basic English sentences, and can be used both for semantic learning, as well as generating grammatically-correct text.
\end{contribution}

\begin{contribution}[Translation Of English Into ASG]
Wrote an ASG learning task capable of taking English text and turning it into set of chronologically-ordered \textit{actions} which convey in ASP what occurs in the text.
\end{contribution}

\begin{contribution}[Automatic Generation Of Summaries]
Developed a set of rules which, given \textit{actions} from a story, allow ASG to generate both \textit{extractive} and \textit{abstractive} summary sentences.
\end{contribution}

\begin{contribution}[Scoring Mechanism]
Implemented a scoring mechanism prioritizing information density, while taking into account words which may appear frequently in English and can be considered the \textit{topic} of the original text.
\end{contribution}