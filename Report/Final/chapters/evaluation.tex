\label{chapter:evaluation}

\section{General Idea}

As the vast majority of modern text summarization frameworks are based on machine learning, it makes sense to compare the performance \textsc{SumASG*} with that of a neural network.

More specifically, we should generate a set of stories which we can give to our framework in order to obtain corresponding summaries. We can then use this as training data for an encoder-decoder, to see if it is able to learn how \textsc{SumASG*} creates summaries.

If the neural network is able to learn to generate similar summaries, then we can consider our framework to be sane.

\section{Story Generation}

\subsection{Libraries}

For this task, we have chosen to use a library called \textbf{\href{http://web.archive.org/web/20190516161631/https://www.clips.uantwerpen.be/pages/pattern-en}{Pattern}}, which allows us to conjugate verbs, as well as toggle nouns between singular and plural.

We also take advantage of the \textbf{\href{https://www.datamuse.com/api/}{Datamuse API}}, which lets us find words which are semantically related to a given word in a certain way.

\subsection{Datasets}

In order to generate the required number of stories, we have used words from \href{http://www.wordfrequency.info/}{wordfrequency.info}. This database contains 5,000 individual English words, of which 1,001 are verbs, 2,542 nouns and 839 adjectives.

For each story we chose a noun from our dataset, which we shall refer to as the \textit{topic}. We then construct four sentences which revolve around this \textit{topic}.

\subsection{Sentence Generation}

We will begin by detailing how each sentence is generated, starting with a few necessary definitions. Throughout this section, it is important to keep in mind that the goal here is to create a story that is as lexically and semantically coherent as possible, which is tricky to do algorithmically.

\subsubsection{Definitions}

\begin{definition}[Hyponym]
A \textit{hyponym} is a word with more specific meaning than another word; ``computer" is a \textit{hyponym} of ``machine".
\end{definition}

\begin{definition}[Hypernym]
A \textit{hypernym} is a semantic superclass of a word; ``vehicle" is a \textit{hypernym} of ``bus".
\end{definition}

\begin{definition}[Holonym]
A \textit{holonym} of something is one of its constituents; ``lightbulb" is a \textit{holonym} of ``lamp".
\end{definition}

\begin{definition}[Meronym]
A \textit{meronym} is an object which something is part of; ``house" is a \textit{meronym} of ``kitchen".
\end{definition}

\subsubsection{Lexical Common Nouns}

Along with the story's \textit{topic}, we also generate a set of \textit{lexical common nouns}. If the \textbf{Datamuse API} is able to find \textit{synonyms} of our \textit{topic} which also belong to our dataset of nouns, then these become the story's \textit{lexical common nouns}.

In addition, we query from the \textbf{Datamuse API} for verbs that are related to the chosen \textit{topic}. This becomes our set of \textit{lexical verbs}. If it is empty, then we make it the singleton set containing the verb ``to be".

Since we don't know how general or specific this randomly selected \textit{topic} is, we may not find any. In this case, we try the same procedure for \textit{hypernyms} and finally \textit{hyponyms}. If we still are unable to find any (which is very rare), then we pick a new random \textit{topic}.

\subsubsection{Subject}

For the \textit{subject} of a sentence, we draw a noun from our \textit{lexical common nouns}.

If this word is singular, then we need a determiner, which can be ``the" or ``a".

We also ask the \textbf{Datamuse API} to find us an adjective which is often modified by the chosen \textit{subject} noun, and is part of our dataset of words. If none is found, then we do not need to use an adjective.

In order to avoid making the data too hard to learn, the same \textit{subject} is used in every sentence of the same story. However, the \textit{verb} and \textit{object} may differ.

\subsubsection{Verb}

We chose a verb at random from our set of \textit{lexical verbs}, conjugating it in the past tense so that it agrees with the sentence's \textit{subject}.

\subsubsection{Object}

For the \textit{object} of our sentence, we look at the \textit{subject} and \textit{verb}. Using the \textbf{Datamuse API} we try and find a noun which often appears after the chosen \textit{verb}, and which is related to our \textit{topic} as well as all nouns we have used thus far in the story. With 50\% probability we ask it to be a \textit{holonym} of the \textit{subject} noun, otherwise it should be a \textit{meronym}.

In the same way as we did for the \textit{subject}, we try and find an adjective often modified by the chosen noun. Sometimes it will be the case that no noun was found, but it is possible in English to have an adjective as the only word in the \textit{object}.

The determiner is added as for the \textit{subject}; if there is no noun we do not use one.

\subsubsection{Example}

We take the example of generating a sentence for a story whose \textit{topic} is ``soccer". In this case, the \textit{lexical common nouns} are a singleton set containing the word ``football". Here also have two \textit{lexical verbs}: ``to match" and ``to pitch".

For the \textit{subject}, we can choose the \textit{lexical common noun} ``football"; an adjective commonly used to modify it is ``professional". Since the noun here is singular, we can use the determiner ``a".

We can then pick ``to pitch" as the \textit{verb}, which becomes ``pitched" when conjugated in the past tense.

For the \textit{object}, we take into account our \textit{topic} ``soccer", to find a \textit{holonym} of ``football" which often appears after the \textit{verb} ``pitched". In this case the \textbf{Datamuse API} returns the word ``reception", resulting in the adjective ``warm" being chosen to accompany it.

After repeating this process three more times, we end up with the below story. As you can see, \textsc{SumASG\textsubscript{2}} would be able to combine the two last sentences, but the result of this may or may not make it into the summary we choose.

\begin{displayquote}
The professional football matched a place. A professional football pitched the warm reception. A professional football matched a warm reception. A professional football matched a wonder.
\end{displayquote}

\subsection{Action Creation}

Using a Python script, we generate the corresponding \textit{actions} as would \textsc{SumASG\textsubscript{1}}, creating the necessary additional leaf nodes for our general grammar in ASG. We do not use \textsc{SumASG\textsubscript{1}} to do this mainly for performance reasons, but also as it is not necessary. Because of the way in which we have created our stories, \textit{simplification} would not change the sentence structure whatsoever, and no sentences would be considered irrelevant (or off-topic) by the \textsc{Preprocessor}.

\subsection{Summary Generation}

For each story, we feed the generated \textit{actions} and leaf nodes directly into \textsc{SumASG\textsubscript{2}}, skipping the first half of the \textsc{SumASG*} pipeline. After \textit{scoring}, we pick an entry at random from the \textit{top summaries}.

\section{Neural Network}

\subsection{Datasets}

Using the mechanism described above, we generate a number of story/summary pairs: 1796 to be used for training, 199 for validation and 10 for testing.

\subsection{Tools}

To allow for greater flexibility, we have chosen to use a highly versatile open-source framework called \textbf{\href{https://github.com/OpenNMT/OpenNMT-py}{OpenNMT-py}} for training our neural network.

In addition, we preprocess the data using Stanford's \textbf{\href{https://nlp.stanford.edu/projects/glove/}{GloVe}} pre-trained word embeddings, giving our network a head-start when it comes to semantics.

\subsection{Encoder-Decoder Architecture}

Our \textit{encoder} and \textit{decoder} share embeddings for a vocabulary of size 4239, its contents being internally represented using a vector of size 500. They both use a two-layer LSTM with \textit{dropout} of 0.25 and \textit{hidden size} of 500. Additionally, our \textit{decoder} uses \textit{global attention}.

\subsection{Training}

The neural network was trained using an Adam optimizer with a \textit{learning rate} of 0.001 and \textit{batch size} of 50. In order to preserve the GloVe word embeddings across training, we fix them at the start and use them for both the \textit{encoder} and \textit{decoder}. Training was done over a period of 10,000 steps (i.e., 200 epochs), validating every 10 epochs. It took 39 epochs for training accuracy to reach 99\%, while validation accuracy slowly increased over time to reach about 53\%.

\textcolor{red}{\textbf{\hl{TODO}}}

\subsection{Results}

At first glance, it may seem as though this \textit{encoder-decoder} was overfitting the training data. However, it is important to keep in mind that multiple valid summaries may exist for a given input story, and that the target summary was not necessarily the same as the one generated by the network.

Unfortunately, due to framework constraints, it is impossible to provide the training script with multiple target summaries. However, it is simple to run the trained network on our test stories, and then compare the results to the many summaries generated by \textsc{SumASG*}, as seen in Figure \ref{fig:neural_network_testing}.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{1.}\\
\textbf{2.}\\
\textbf{3.}\\
\textbf{4.}\\
\end{displayquote}
\caption{Test stories}
\end{subfigure}
\begin{subfigure}{\textwidth}
\vspace{\baselineskip}
\begin{displayquote}
\textbf{1.}\\
\textbf{2.}\\
\textbf{3.}\\
\textbf{4.}\\
\end{displayquote}
\caption{Summaries generated by the \textit{encoder-decoder}}
\end{subfigure}
\begin{subfigure}{\textwidth}
\vspace{\baselineskip}
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
BLEU score                                        & 1 & 2 & 3 & 4 \\ \midrule
Predicted summary                                 & 0 & 0 & 0 & 0 \\
Closest \textsc{SumASG*} summary & 0 & 0 & 0 & 0 \\ \bottomrule
\end{tabular}
\caption{Maximum BLEU scores against original story}
\end{subfigure}
\caption{Evaluation results}
\label{fig:neural_network_testing}
\end{figure}

\textcolor{red}{\textbf{\hl{TODO}}}

\section{Takeaways}

To sum up what we have learned from this experiment, we can use a table to outline the main differences between our approach and using a neural network, as shown in Figure \ref{fig:takeaways}.

\begin{table}[H]
\centering
\begin{tabular}{@{}L{0.25\textwidth}L{0.4\textwidth}L{0.35\textwidth}@{}}
\toprule
                                        & Neural network                                                                                              & \textsc{SumASG*}                        \\ \midrule
Rules                                   & Learnable using state-of-the-art \textit{encoder-decoders}                                 & Written directly into program  \\
Training required                       & Yes; can take a long time                                                                                   & No                             \\
Examples required                       & Vast amounts for training                                                                                   & None                           \\
Expansion                               & Need to retrain                                                                                             & Can be used directly           \\
Coherence of result                     & Extremely tied to nature and diversity of training corpus                                                   & Similar on all parsable texts  \\
Output \textit{tokens} & Can be irrelevant or \texttt{<unk>} \textit{token} & Always taken from input text   \\
Termination                             & Always produces output, regardless of whether input is valid English                                        & Sometimes returns no summaries \\ \bottomrule
\end{tabular}
\caption{Main differences between \textsc{SumASG*} and neural networks used for the task of text summarization}
\label{fig:takeaways}
\end{table}