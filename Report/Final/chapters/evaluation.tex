\label{chapter:evaluation}

In this chapter we present the validation and evaluation of our approach, which involves comparing \textsc{SumASG*} to an \textit{encoder-decoder}. After explaining the reasoning behind this comparison (Section \ref{sec:evaluation_general_idea}), we present the training data that are used for validation (Section \ref{sec:story_generation}) and how our neural network performs on them (Section \ref{sec:validation}). We then carry out two evaluation experiments (Section \ref{sec:evaluation_experiments}) and finish by giving an overview of what we have learned (Section \ref{sec:evaluation_takeaways}).

\section{General Idea}
\label{sec:evaluation_general_idea}

As the vast majority of modern text summarization frameworks are based on Machine Learning, it makes sense to evaluate \textsc{SumASG*} against a neural network. There exist a number of text summarization corpora, such as the \textbf{CNN / Daily Mail dataset}\footnote{The CNN / Daily Mail dataset consists of around 300,000 news articles and their corresponding multi-sentence summaries; these have been used to train state-of-the-art text summarization models, which were evaluated using metrics such as ROUGE and METEOR: \url{http://nlpprogress.com/english/summarization.html}}. However, we cannot use one of these datasets with \textsc{SumASG*} as it is unfortunately not general enough to be able to parse such complicated text.

In order to validate our approach, we have built a generative model which creates stories that our system is capable of summarizing. Using \textsc{SumASG*}, we generate summaries corresponding to these stories, and use this as training data for an \textit{encoder-decoder}.

To evaluate our approach, we then use this generated dataset as a starting point to check both the \textit{encoder-decoder} and \textsc{SumASG*} for robustness to small perturbations in the input, as well as their ability to detect invalid input.

\section{Story Generation}
\label{sec:story_generation}

To give a more generalized representation of what \textsc{SumASG*} can do, we shall create two different types of stories: those with \textit{conjunctive summaries}, and those with \textit{descriptive summaries}. In this section, we go more into depth about these two types of randomly-generated stories, and discuss how they are created.

\subsection{Datasets}

In order to generate the required number of stories, we have used words from \textbf{wordfrequency.info}\footnote{\url{wordfrequency.info} hosts a dataset of the most commonly used words in English}. This dataset contains 5,000 individual English words, of which 1,001 are verbs, 2,542 nouns and 839 adjectives.

For each story we chose a noun from our dataset, which we shall refer to as the \textit{topic}. We will later construct sentences that revolve around this \textit{topic}.

In addition, we query from the \textbf{Datamuse API}\footnote{Datamuse is a lexical knowledge engine which can be used to find words that are semantically related to a given word in a certain way: \url{https://www.datamuse.com/api/}} for what we call a \textit{lexical verb}, i.e. one that is related to the story's \textit{topic}. If one cannot be found, then we default to the verb ``to be".

\subsection{Main Sentence Generation}

We will begin by detailing how what we call \textit{main sentences} are generated, starting with a few necessary definitions. Throughout this section, it is important to keep in mind that the goal here is to create a story that is as lexically and semantically coherent as possible, which is tricky to do algorithmically. It is important to note that all \textit{main sentences} for the same story share a common \textit{subject} and \textit{verb}.

\begin{definition}[Holonym]
A \textit{holonym} of something is one of its constituents; ``lightbulb" is a \textit{holonym} of ``lamp".
\end{definition}

\begin{definition}[Meronym]
A \textit{meronym} is an object which something is part of; ``house" is a \textit{meronym} of ``kitchen".
\end{definition}

\noindent
For the \textit{subject} of a \textit{main sentence}, we use the story's \textit{topic}. This being a singular noun, we need to add a determiner, which can be ``the" or ``a". We also ask the \textbf{Datamuse API} to find us an adjective which is often modified by the chosen \textit{subject} noun, and is part of our dataset of words. If none are found, then we do not need to use an adjective.

Here we use the \textit{lexical verb}, conjugating it in the past tense using \textbf{Pattern}\footnoteref{footnote:pattern} so that it agrees with the sentence's \textit{subject}.

For the \textit{object} of our sentence, we look at the story's \textit{topic} and \textit{lexical verb}. Using the \textbf{Datamuse API} we try and find a noun which often appears right after this verb, and which is related to all of the nouns we have used thus far in the story. With 50\% probability we ask it to be a \textit{holonym} of the \textit{topic}, otherwise it should be a \textit{meronym}. In the same way as we did for the \textit{subject}, we try and find an adjective often modified by the chosen noun. Sometimes it will be the case that no noun was found, but it is fine in English to have an adjective as the only word in the \textit{object}. The determiner is added as for the \textit{subject}; if there is no noun we do not use one.

\subsubsection*{Example}

We take the example of generating a sentence for a story whose \textit{topic} is ``football". In this case, we have two choices for our \textit{lexical verb}: ``to match" and ``to pitch".

For the \textit{subject}, we use the \textit{topic} ``football" with the determiner ``a"; an adjective commonly used to modify this word is ``professional". We then conjugate the verb ``to pitch" in the past tense, which becomes ``pitched". For the \textit{object}, we take into account our \textit{topic} ``football" to find a \textit{holonym} of this word which often appears after the \textit{verb} ``pitched". In this case the \textbf{Datamuse API} returns the word ``reception", resulting in the adjective ``warm" being chosen to accompany it.

The resulting \textit{main sentence} can be seen in Figure \ref{fig:main_sentences_example}, along with a second \textit{main sentence} that has been generated for the same story.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
A professional football pitched the warm reception. \\
A professional football pitched a place.
\end{displayquote}
\end{subfigure}
\caption{\textit{Main sentences} generated for a story with \textit{topic} ``football" and \textit{lexical verb} ``to pitch"}
\label{fig:main_sentences_example}
\end{figure}

\noindent
As has already been mentioned, generating a coherent and meaningful text is a very difficult problem for computers. This explains why the \textit{main sentences} shown in Figure \ref{fig:main_sentences_example} do not make much sense from a human perspective. However, they are both grammatically correct, as well as highly suitable for the task of summarization, which is what we are looking for in this experiment.

\subsection{Conjunctive Summaries}

Stories with \textit{conjunctive summaries} consist of three \textit{main sentences}. Because of the way in which we have implemented the summarization rules in \textsc{SumASG}, one of the sentences in the corresponding summary should be a combination of two of the input story's sentences. That is to say, its \textit{object} should consist of these two sentences' \textit{objects}, joined using the conjunction ``and". An example is shown in Figure \ref{fig:conjunctive_summary_example}.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
The publication printed a lightning. The publication printed a movement. The publication printed the stereotype.
\end{displayquote}
\caption{Story}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
The publication printed a lightning \textbf{and} a movement. The publication printed the stereotype.
\end{displayquote}
\caption{Summary}
\end{subfigure}
\caption{Example of a story with a \textit{conjunctive summary}}
\label{fig:conjunctive_summary_example}
\end{figure}

\subsection{Descriptive Summaries}

In contrast, stories with a \textit{descriptive summary} consist of a single \textit{main sentence}, one which \textit{does} contain an adjective in the \textit{object} position.

We use the second of its two sentences to expand on the first. To be more precise, the \textit{object} of this sentence is the same as in the first, apart from the fact that we assign an adjective as is done for typical \textit{main sentences}. However the \textit{subject} here is the preposition ``it", while the \textit{verb} is the verb ``to be" conjugated in the past tense.

The idea for a \textit{descriptive summary} is that it will be identical to the first sentence of its corresponding story, but augmented with the adjective coming from the second sentence, as illustrated in Figure \ref{fig:descriptive_summary_example}.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
The heavy traffic transported the birthday. It was the isolated birthday.
\end{displayquote}
\caption{Story}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
The heavy traffic transported the isolated birthday.
\end{displayquote}
\caption{Summary}
\end{subfigure}
\caption{Example of a story with a \textit{descriptive summary}}
\label{fig:descriptive_summary_example}
\end{figure}

\subsection{Summary Generation}

Using a Python script, we generate the corresponding \textit{actions} as would \textsc{SumASG\textsubscript{1}}, creating the necessary additional leaf nodes for our general grammar in ASG. We do not use \textsc{SumASG\textsubscript{1}} to do this mainly for performance reasons, but also because we can consistently produce the same \textit{actions} as \textsc{SumASG\textsubscript{1}} programmatically with the chosen sentence structure. Also, because of the way in which we have created our stories, \textit{simplification} in the \textsc{Preprocessor} would not change anything whatsoever.

To generate a summary for this experiment, we take a story and feed the corresponding \textit{actions} and leaf nodes directly into \textsc{SumASG\textsubscript{2}}, skipping the first half of the \textsc{SumASG*} pipeline. After \textit{scoring}, we pick an entry at random from the \textit{top summaries}.

\section{Validation}
\label{sec:validation}

Using the mechanism described in Section \ref{sec:story_generation}, we generate for our \textit{encoder-decoder} 4,000 story/summary pairs: 3,582 to be used for training, 398 for validation and 20 for testing.

To allow for greater flexibility, we have chosen to use \textbf{OpenNMT-py}\footnote{OpenNMT-py is a highly versatile open-source framework for performing Neural Machine Translation: \url{https://github.com/OpenNMT/OpenNMT-py}} to train our neural network. In addition, we preprocess the data using \textbf{GloVe}\footnote{GloVe is a Machine Learning model consisting of pre-trained word embeddings: \url{https://nlp.stanford.edu/projects/glove/}}, giving our network a head start when it comes to semantics.

Our \textit{encoder} and \textit{decoder} share embeddings for a vocabulary of size 4,239, its contents being internally represented using a vector of size 500. They both use a two-layer LSTM with \textit{dropout} of 0.25 and \textit{hidden size} of 500. Additionally, our \textit{decoder} uses \textit{global attention}.

The neural network was trained using an Adam optimizer with a \textit{learning rate} of 0.001 and \textit{batch size} of 25. In order to preserve the GloVe word embeddings across training, we fix them at the start and use them in both the \textit{encoder} and \textit{decoder}.

Training was done over a period of 10,000 steps (i.e., 400 epochs), validating every 20 epochs. Using \textbf{Google Colaboratory}\footnote{Google Colaboratory is an online tool for creating Python notebooks, providing free access to GPUs: \url{https://colab.research.google.com/}} this took a total of 5 minutes and 30 seconds. The final training and validation accuracies are respectively 99.83\% and 92.07\%, as shown in Table \ref{table:training_metrics}.

\begin{table}[H]
\centering
\begin{tabular}{@{}llllll@{}}
\toprule
Epochs                & 20 & 100 & 200 & 300 & 400 \\ \midrule
Training accuracy (\%)     & 60.68   & 98.73   & 99.45    & 99.82    & 99.83    \\
Validation accuracy (\%)   & 61.54   & 90.24   & 91.38    & 91.38    & 92.07    \\
Training perplexity   & 12.45   & 1.06   & 1.03    & 1.01    & 1.01    \\
Validation perplexity & 16.73   & 2.07   & 2.15    & 2.23    & 2.36    \\
Test perplexity & -   & -   & -    & -    & 2.05    \\ \bottomrule
\end{tabular}
\caption{Evolution of accuracy and perplexity during training}
\label{table:training_metrics}
\end{table}

\subsection*{Discussion}

Unsurprisingly, our \textit{encoder-decoder} was able to mostly learn this hard-coded summarization task, as confirmed by the numbers in Table \ref{table:training_metrics}. This is because neural networks have a much more general scope than \textsc{SumASG*}.

However, it appears that it was not able to fully learn the rules used by \textsc{SumASG}, as the final validation and test perplexities are a little high ($> 2$).

Part of this discrepancy is due to the use of slightly different words in the predicted summaries. Upon further inspection, the words that differ have very close meanings; this is because we have used pre-trained embeddings and fixed their representation. It is not too problematic though, as it would be much worse to get a semantically-unrelated word in one of our predicted summaries.

Another reason for this discrepancy is the fact that multiple \textit{conjunctive summaries} can be generated from the same story, depending on which words are chosen to be put together.

An example of a predicted summary which highlights both of these discrepancies is shown in Figure \ref{fig:discrepancy_example}.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
A new approach attacked the \underline{lesson}. A new approach attacked the \underline{counseling}. A new approach attacked the \underline{league}.
\end{displayquote}
\caption{Story}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
A new approach attacked the \underline{lesson} \textbf{and} the \underline{counseling}. A new approach attacked the \underline{league}.
\end{displayquote}
\caption{Expected summary}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
A new approach attacked the \underline{lesson} \textbf{and} the \underline{league}. A new approach attacked the \underline{patient}.
\end{displayquote}
\caption{Predicted summary}
\end{subfigure}
\caption{Example of a story whose predicted summary is different from the expected summary, but with very close semantics}
\label{fig:discrepancy_example}
\end{figure}

\section{Evaluation Experiments}
\label{sec:evaluation_experiments}

In order to evaluate our approach and highlight some of the major differences between \textsc{SumASG*} and a neural network-based text summarization system, we carry out two experiments in what follows.

\subsection{Experiment 1: Robustness To Perturbations}
\label{subsec:experiment_1}

For the first experiment, we take the test data from Section \ref{sec:validation}, which we know works well with our \textit{encoder-decoder} (see Table \ref{table:training_metrics}), and apply a few small perturbations to the stories, while keeping them grammatically correct. With these changes, we now run both text summarization systems to see how they perform.

To avoid giving \textsc{SumASG*} an advantage, let us choose a story whose predicted summary is exactly the expected summary. Also, to prevent sentences in the modified stories from being pruned, we bypass the \textsc{Preprocessor}.

For perturbation \textbf{A} we move the \textit{subject}'s adjective to each \textit{object} in the story, while perturbation \textbf{B} involves exchanging the \textit{subject} and \textit{object} in the first sentence of the story.

The results of applying these two perturbations are shown below in Figure \ref{fig:experiment_1}.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
The nervous system processed a fraud. The nervous system processed a whale. The nervous system processed the American.
\end{displayquote}
\caption{Original story}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{A.} The system processed a \underline{nervous} fraud. The system processed a \underline{nervous} whale. The system processed the \underline{nervous} American. \\
\textbf{B.} \underline{A fraud} processed \underline{the nervous system}. The nervous system processed a whale. The nervous system processed the American.
\end{displayquote}
\caption{Story with perturbations \textbf{A} and \textbf{B} respectively applied (changes are underlined)}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{A.} The system processed a banking and a killer. The system processed the nervous. \\
\textbf{B.} A fraud processed the nervous and the American. A fraud processed a whale.
\end{displayquote}
\caption{Summaries predicted by the trained \textit{encoder-decoder}}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{A.} The system processed a fraud and a whale. The system processed the nervous American. \\
\textbf{B.} The nervous system processed the American. The nervous system processed a whale.
\end{displayquote}
\caption{Summaries generated by \textsc{SumASG*} (without using the \textsc{Preprocessor})}
\end{subfigure}
\caption{Results of experiment 1: summaries generated by \textsc{SumASG*} and the \textit{encoder-decoder} after separately applying two different perturbations}
\label{fig:experiment_1}
\end{figure}

\noindent
Unsurprisingly, \textsc{SumASG*} is able to produce a grammatically-correct summary with little loss of information and no change in meaning for both perturbations.

However, our neural network has created a \textit{conjunctive summary} for perturbation \textbf{A} that changes the first two \textit{objects} to words with a slight semantic link, and makes too much of a generalisation of the third sentence of the modified story by omitting the \textit{object} noun. For perturbation \textbf{B}, the summary is even more different from the modified story: \textit{subjects} and \textit{objects} from different sentences have been put together, conveying a completely different meaning. Of course, this is extremely tied to the data we have used to train our \textit{encoder-decoder}, so such results were expected.

From this experiment we conclude that \textsc{SumASG*} is more robust to small changes in the input than a neural network (with respect to training corpus), as well as much more consistent when it comes to producing a summary that is coherent with the original text.

\subsection{Experiment 2: Input Validity Awareness}
\label{subsec:experiment_2}

The goal of the second experiment is to highlight how both systems deal with invalid input (i.e., stories that are not grammatically correct). To avoid giving \textsc{SumASG*} too much of an advantage, we use only words from the vocabulary known by our \textit{encoder-decoder}, and create sequences of similar length compared to the training data used in Section \ref{sec:validation}.

For invalid input \textbf{A} we take the original story from Figure \ref{fig:experiment_1} and make a few small changes to render it grammatically incorrect, while for invalid input \textbf{B} we create a story of length 3 from scratch using randomly selected words from the vocabulary.

While \textsc{SumASG*} is able the recognise that these stories are not grammatically correct, our \textit{encoder-decoder} produces an output, as is shown in Figure \ref{fig:experiment_2}.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{A.} The system processed \underline{processed} a nervous fraud. \underline{processed} The system a nervous whale. \underline{a} The system processed nervous American. \\
\textbf{B.} question hunger whole ruled cleared. needle front pound spun rented programming. bought journalism disclosed broad check died delight.
\end{displayquote}
\caption{Invalid stories (changes in story \textbf{A} from the original story in Figure \ref{fig:experiment_1} are underlined; story \textbf{B} is generated from arbitrary words in the \textit{encoder-decoder}'s vocabulary)}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{A.} The system processed a nervous and a killer. The system processed a pregnancy. \\
\textbf{B.} The
\end{displayquote}
\caption{Summaries predicted by the trained \textit{encoder-decoder}}
\end{subfigure}
\caption{Results of experiment 2: summaries generated by the \textit{encoder-decoder} on invalid input stories (\textsc{SumASG*} recognises these as invalid, producing no output)}
\label{fig:experiment_2}
\end{figure}

\noindent
Since \textsc{SumASG*} is built around a grammar that models the structure of English sentences, the fact that it recognises these stories as invalid is no surprise.

By looking at the neural network's prediction for invalid story \textbf{A}, we see that only one of the three nouns in the output summary appear in the story. Moreover, even if we were to correct story \textbf{A}, its meaning would have nothing in common with the predicted summary.

In contrast, the prediction for invalid story \textbf{B} is not grammatically correct, and its only word does not even appear in the story. Of course, such a result is expected as this story is incredibly different from any of the training pairs, and does not make any sense whatsoever.

From this experiment, we can conclude that a neural network trained for text summarization cannot detect when the input is invalid, unless it has been specifically trained to do so. However, \textsc{SumASG*} is by construction unable to summarize grammatically-incorrect stories.

\section{Takeaways}
\label{sec:evaluation_takeaways}

To sum up what we have learned from validating and then evaluating our approach, we use a table to outline the main differences between \textsc{SumASG*} and using a neural network, as shown in Table \ref{table:takeaways}.

\begin{table}[H]
\centering
\begin{tabular}{@{}L{0.25\textwidth}L{0.4\textwidth}L{0.35\textwidth}@{}}
\toprule
                                        & Neural network                                                                                              & \textsc{SumASG*}                        \\ \midrule
Rules                                   & Learnable using state-of-the-art \textit{encoder-decoders}                                 & Written directly into program  \\
Training required                       & Yes; can take a long time                                                                                   & No                             \\
Examples required                       & Vast amounts for training                                                                                   & None                           \\
Expansion                               & Need to retrain                                                                                             & Can be used directly           \\
Coherence of result (Subsection \ref{subsec:experiment_1})                     & Extremely tied to nature and diversity of training corpus                                                   & Similar on all parsable texts  \\
Output \textit{tokens} & Can be irrelevant or \texttt{<unk>} \textit{token} & Taken from input text   \\
Termination\mbox{    } (Subsection \ref{subsec:experiment_2})                             & Always produces output, regardless of whether input is valid English                                        & Sometimes returns no summaries \\ \bottomrule
\end{tabular}
\caption{Main differences between \textsc{SumASG*} and neural networks used for the task of text summarization}
\label{table:takeaways}
\end{table}