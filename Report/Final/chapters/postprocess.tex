\section{Overview}

Once we have obtained potential sentences from ASG to be used in a summary, we can now post-process these as explained in Section \ref{sec:summary_creation}. By combining them in different ways, we are able to form summaries. From these, we will retain the highest scoring ones, according to the metric detailed in Section \ref{sec:scoring}. A diagram illustrating these steps is shown below in Figure \ref{fig:postprocess_pipeline}.

\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=0.55cm, auto]
\node (summary_sentence_1) [block] {Summary Sentence 1};
\node (summary_sentence_2) [block, below =of summary_sentence_1] {Summary Sentence 2};
\node (summary_sentence_3) [below =of summary_sentence_2] {...};
\node (summary_sentence_4) [block, below =of summary_sentence_3] {Summary Sentence n};
\node (post_process_1) [block, right =of summary_sentence_1] {Post-Process};
\node (post_process_2) [block, below =of post_process_1] {Post-Process};
\node (post_process_3) [below =of post_process_2] {...};
\node (post_process_4) [block, below =of post_process_3] {Post-Process};
\node (combine) [block, right =of post_process_2] {Combine};
\node (summary_1) [block, above right =of combine] {Summary 1};
\node (summary_2) [block, below =of summary_1] {Summary 2};
\node (summary_3) [below =of summary_2] {...};
\node (summary_4) [block, below =of summary_3] {Summary m};
\node (score_1) [right =of summary_1] {Score};
\node (score_2) [below =of score_1, right =of summary_2] {Score};
\node (score_3) [below =of score_2] {...};
\node (score_4) [below =of score_3, right =of summary_4] {Score};
\draw [->] (summary_sentence_1) -- (post_process_1);
\draw [->] (summary_sentence_2) -- (post_process_2);
\draw [->] (summary_sentence_4) -- (post_process_4);
\draw [->] (post_process_1) -- (combine);
\draw [->] (post_process_2) -- (combine);
\draw [->] (post_process_4) -- (combine);
\draw [->] (combine) -- (summary_1);
\draw [->] (combine) -- (summary_2);
\draw [->] (combine) -- (summary_4);
\draw [->] (summary_1) -- (score_1);
\draw [->] (summary_2) -- (score_2);
\draw [->] (summary_4) -- (score_4);
\end{tikzpicture}
\caption{Post-Processing / Scoring Steps}
\label{fig:postprocess_pipeline}
\end{figure}

\section{Summary Creation}
\label{sec:summary_creation}

The output of \textsc{SumASG} is a list of sentences, each of which could potentially appear in the final summary.

\subsection{Post-Processing}

\subsubsection{Grammar}

Because \textsc{SumASG} uses the same capitalization for a given word regardless of its position in the sentence, it means that the first word of each sentence will not be capitalized unless it is a proper noun. We therefore need to fix this, as well as remove the space before each full stop.

Compound nouns, whose hyphen was replaced with an underscore for the internal representation of \textsc{SumASG}, also need to be restored to their grammatically-correct form.

In addition, the task of summarization might have created a sentence where an incorrect verb form is used, or possibly the wrong determiner. To amend this we use a tool called \textbf{\href{https://pypi.org/project/language-check/}{language-check}}, which is able to correct phrases like ``they has an dog" to ``they have a dog".

\subsubsection{Complex Nouns}

One of the optimizations done by the \textsc{Preprocessor} was to combine complex nouns such as ``Peter Little" into their camel-case form ``PeterLittle", so that they would be recognized as a single token by \textsc{SumASG}. We now need to expand them back to their original form, as this is how it should be written in English.

\subsection{Combining}

Depending on the length of the original story, we can envision and different number of sentences to be in the summary, as shown below in Table \ref{tab:summary_length}.

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Story length   & 1-2 & 3-4 & 5+ \\ \midrule
Summary length & 1   & 2   & 3  \\ \bottomrule
\end{tabular}
\caption{Length of a summary depending on the number of sentences in the story}
\label{tab:summary_length}
\end{table}

Once we have grammatically-correct summary sentences and know how many should be kept for the summary (say $n$), we generate all possible order-preserving combinations of length $n$. For instance, such combinations of length 3 for the list $[0,1,2,3]$ would be the following: $[0,1,2]$, $[0,2,3]$ and $[1,2,3]$.

\section{Scoring}
\label{sec:scoring}

As you can imagine, we often end up with a large number of combinations at this phase. We therefore need to determine which of them are best and keep only these.

\subsection{TTR}

To this end, we utilize an NLP metric called TTR (type-token ratio), a measure of lexical density. To provide the most informative summaries possible, we want to maximize the density of unique words.

To calculate a summary's TTR, we divide the number of unique words in the summary by the total number of words. We then divide this by number of unique words in the story and multiply it by a constant, in order to get a more consistent range for our scores.

\subsubsection{Ignored Words}

However, we do not want to neglect summaries using the same determiner, proper noun, or the verb ``to be" multiple times, as these are extremely common in English.

In addition, a story might revolve around a given topic and/or character. Regarding the former, it could also be the case that the \textsc{Preprocessor} had replaced different synonyms of this topic with a unique word.

To get around this, what we can do is to exclude such words from the summary length and number of unique summary words. This way, we no longer require that these ``common" words we unique in a summary. In the following, we will call this score TTR*.

In Figure \ref{fig:score_example} is an example which illustrates this metric. The summary with the highest final score is considered to be the best. As you can see, there is a greater difference between the summaries when using TTR*, which takes into account the commonly-used building blocks of the English language.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
Jonathan was a little boy. He was hungry. Jonathan was eating an apple.
\end{displayquote}
\caption{Story}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\begin{displayquote}
\textbf{A.} \underline{Jonathan} \underline{was} \underline{a} hungry boy. \underline{Jonathan} \underline{was} eating \underline{an} apple.\\
\textbf{B.} \underline{Jonathan} \underline{was} \underline{a} little boy. \underline{Jonathan} \underline{was} \underline{a} hungry boy.
\end{displayquote}
\caption{Possible summaries (underlined words will be ignored by TTR*)}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{@{}llllllll@{}}
\toprule
Summary & Words & Unique words & TTR & Words* & Unique words* & TTR* & Score \\ \midrule
\textbf{A} & 10    & 8            & 0.8 & 4      & 4             & 1    & 38    \\
\textbf{B} & 10    & 6            & 0.6 & 4      & 3             & 0.75 & 28    \\ \bottomrule
\end{tabular}
\end{subfigure}
\caption{Score computation (column headers ending with * pertain to TTR*)}
\label{fig:score_example}
\end{figure}

\section{Summary Selection}

\subsection{Proper Nouns}

If a story revolves around a given person and the summary mentions their name, it is preferable for this to be in the first sentence. To put this more clearly, we would like the summary of a biography to introduce the protagonist from the very first sentence. To achieve this, we can simply increase the score of every summary starting with a proper noun by a constant.

\subsection{Top Summaries}

With a more complex story (5 or more sentences), it is highly likely that we will end up with a very long list of possible summaries. As there could be a number of very interesting summaries, we do not want to have to choose exactly one.

Instead, we compute 75th percentile over the scores of all generated summaries, and then discard all those whose score falls below this number. We shall call the remaining summaries \textit{top summaries}.

\subsection{Reference Summaries}

Finally, we want to be sure that our framework generates good summaries, and that the scoring works as intended. Therefore, if a story has a reference summary, then we should make sure that there exists a similar \textit{top summary}.

In our implementation, we have chosen to use the BLEU score, which measures how closely the output given by a machine matches a text written by a human. If there exists a \textit{top summary} whose BLEU score with one of the references is above a certain threshold, then we consider the summarization to be successful.

\section{Example}
\label{sec:postprocess_example}

\textcolor{red}{\textbf{\hl{TODO}}}

\section{Expandability}

As you can see from the example in Section \ref{sec:postprocess_example}, there is much room for improvement regarding post-processing.

\subsection{Grammatical Shortcomings}

First of all, we do not revert all the simplification changes made by the \textsc{Preprocessor}. This can lead to a linguistically poor summary, where the same word or name is repeated multiple times, rather than using synonyms or personal pronouns.

Worse than this, we can end up with sentences that would never be written by a human. Because the \textsc{Preprocessor} moves all adverbs to the end of the sentence in which they appear, and is quite eager to homogenize synonyms, summaries generated by \textsc{SumASG*} may end up ``sounding wrong".

\subsection{Better Summary Selection}

Another issue is that we can easily end up with a very large list of summaries. Because the mechanism used to score them is not very advanced, it cannot say for sure that one particular summary is better than all the others. Instead, we usually end up with multiple entries that all have the same maximum score.

We would therefore need to build much more intelligence into this system if we wanted the program to always return a single summary, one that is humans would also consider optimal.