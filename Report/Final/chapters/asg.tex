\label{chapter:asg}

\section{Overview}

Our use of ASG is two-fold. Firstly, we pass in each sentence from the story to ASG to obtain its semantic representation in ASP. Secondly, we take these \textit{actions} and use ASG rules to generate possible summary components. These will later be post-processed and turned into actual valid summaries. A diagram of the two ASG steps is shown below in Figure \ref{fig:asg_pipeline}.

{
\floatstyle{plain}
\restylefloat{figure}
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=0.55cm, auto]
\node (sentence_1) [inter] {Sentence 1};
\node (sentence_2) [inter, below =of sentence_1] {Sentence 2};
\node (sentence_3) [below =of sentence_2] {...};
\node (sentence_4) [inter, below =of sentence_3] {Sentence n};
\node (learn_action_1) [block, right =of sentence_1] {Learn Action};
\node (learn_action_2) [block, right =of sentence_2, below =of learn_action_1] {Learn Action};
\node (learn_action_3) [right =of sentence_3, below =of learn_action_2] {...};
\node (learn_action_4) [block, right =of sentence_4, below =of learn_action_3] {Learn Action};
\node (gen_summaries) [block, right =of learn_action_2] {Generate Summaries};
\node (summary_sentence_1) [inter, above right =of gen_summaries] {Summary Sentence 1};
\node (summary_sentence_2) [inter, right =of gen_summaries, below =of summary_sentence_1] {Summary Sentence 2};
\node (summary_sentence_3) [right =of gen_summaries, below =of summary_sentence_2] {...};
\node (summary_sentence_4) [inter, right =of gen_summaries, below =of summary_sentence_3] {Summary Sentence m};
\draw [->] (sentence_1) -- (learn_action_1);
\draw [->] (sentence_2) -- (learn_action_2);
\draw [->] (sentence_4) -- (learn_action_4);
\draw [->] (learn_action_1) -- (gen_summaries);
\draw [->] (learn_action_2) -- (gen_summaries);
\draw [->] (learn_action_4) -- (gen_summaries);
\draw [->] (gen_summaries) -- (summary_sentence_1);
\draw [->] (gen_summaries) -- (summary_sentence_2);
\draw [->] (gen_summaries) -- (summary_sentence_4);
\end{tikzpicture}
\caption{ASG Steps}
\label{fig:asg_pipeline}
\end{figure}
}

\section{Internal Representation}
\label{sec:internal_representation}

In order to model the structure of sentences the English language, we have created a CFG that has a similar hierarchy to that of an NLP parse tree. The ASG code for this general structure can be seen in Appendix \ref{appendix:asg}. Throughout this description of \textsc{SumASG}, please refer to Chapter \ref{chapter:background} for information on how to interpret an ASG program. Also, a table listing the possible POS tags is available in Appendix \ref{appendix:pos}.

\subsection{Leaf Nodes}

At the bottom end of the structure, there are leaf nodes that correspond to individual English words. These nodes are added based on the context, that is to say the words appearing in our story.

Each of these nodes has on the left-hand side (LHS) of the derivation its POS tag, and on the right-hand side (RHS) a string containing the word itself. In order to conform to the syntax of ASG, we must write the POS tags in lower-case. Also, we include a space at the end of each word's textual representation so that when we run our program the words appear distinct and not all concatenated together.

In ASG every derivation also has a set of ASP rules, which in the case of leaf nodes is just a single rule telling us the word's \textit{lemma} and sentence \textit{role}. In the case of verbs, the \textit{lemma} is simply the base form of the verb, so we also need to keep track of its tense.

For example, leaf nodes for the sentence ``they drove a race-car fast." would look like this:

\begin{displayquote}
\begin{lstlisting}
prp -> ``they " { noun(they). }
vbd -> ``drove " { verb(drive,past). }
dt -> ``a " { det(a). }
nn -> ``race-car " { noun(race_car). }
rb -> ``fast " { adj_or_adv(fast). }
\end{lstlisting}
\end{displayquote}

As part of the input to \textsc{SumASG}, we receive some leaf nodes corresponding to words in the story, where the \textit{lemmas} and \textit{roles} have been assigned by the \textsc{Preprocessor}.

In Figure \ref{fig:leaf_nodes}, you can see which POS tags fall under which \textit{roles}, keeping in mind that this categorization is only an optimization and was not intended to strictly adhere to English grammar.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textit{Role}         & POS tags                    \\ \midrule
\texttt{verb(\underline{lemma},\underline{tense})}         & VB, VBD, VBG, VBN, VBP, VBZ \\
\texttt{noun(\underline{lemma})}         & EX, NN, NNS, NNP, NNPS, PRP \\
\texttt{det(\underline{lemma})}          & CD, DT, IN                  \\
\texttt{adj\_or\_adv(\underline{lemma})} & JJ, JJR, JJS, RB, RP        \\ \bottomrule
\end{tabular}
\caption{POS tags by \textit{role}}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{@{}lllllll@{}}
\toprule
POS tag  & VB & VBD  & VBG    & VBN        & VBP     & VBZ            \\ \midrule
Verb tense & \texttt{base} & \texttt{past} & \texttt{gerund} & \texttt{past\_part} & \texttt{present} & \texttt{present\_third} \\ \bottomrule
\end{tabular}
\caption{Verb tense by POS tag}
\vspace{\baselineskip}
\end{subfigure}
\caption{Predicates used for the leaf nodes in the internal representation}
\label{fig:leaf_nodes}
\end{figure}

\subsection{Non-Leaf Nodes}

The job of the non-leaf nodes is to join leaf nodes together, matching the way we would join words in English to form a sentence.

In our general grammar, sentences (\texttt{s -> np vp}) are made up a \textit{noun part} (\texttt{np}) followed by a \textit{verb part} (\texttt{vp}). While a \textit{noun part} can be made up of leaf nodes, a \textit{verb part} is always a verb followed by a \textit{noun part}.

\subsubsection{Noun Parts}

In the derivation for a \textit{noun part}, we use logic rules whose role it is to encapsulate a sentence \textit{subject} and/or an \textit{object}. This is done in a bottom-up manner, by using information from the child node(s) to populate a predicate at the \textit{noun part} level.

The reason why we differentiate between these two forms is because some \textit{noun parts} in English are only used as subjects (e.g., existential ``there"), while others can only be objects (e.g., the adjective ``green"), and we want to keep the \textit{search space} as small as possible (see Subsection \ref{subsec:search_space}).

The resulting predicates have respective forms \texttt{subject(\underline{noun},\underline{det},\underline{adj\_or\_adv})} and \texttt{object(\underline{noun},\underline{det},\underline{adj\_or\_adv})}. For all these predicates, we use the \textit{ground term} \texttt{0} to denote the absence of a \textit{token}.

For instance, we can capture the \textit{noun phrase} ``a race-car" using the following derivation:

\begin{displayquote}
\begin{lstlisting}
np -> dt nn {
  subject(N,D,0) :- det(D)@1, noun(N)@2.
  object(N,D,0) :- det(D)@1, noun(N)@2.
}
\end{lstlisting}
\end{displayquote}

To handle the case of more complex \textit{noun phrases}, we have created a special predicate \texttt{conjunct(\underline{first},\underline{second})}, allowing us to join two words with the same \textit{role}. We also need to add constraints that rule out cases where the two \textit{conjuncts} have the same \textit{lemma}.

For example, the \textit{noun phrase} ``bread and cheese" would be encompassed by the below derivation:

\begin{displayquote}
\begin{lstlisting}
np -> nn ``and " nn {
  subject(conjunct(N1,N2),0,0) :- noun(N1)@1, noun(N2)@3.
  object(conjunct(N1,N2),0,0) :- noun(N1)@1, noun(N2)@3.
  :- subject(conjunct(N,N),0,0).
  :- object(conjunct(N,N),0,0).
}
\end{lstlisting}
\end{displayquote}

\subsubsection{Verb Parts}

The last child node of a \textit{verb part} is always a single \textit{noun part}. Before that comes a verb, whose POS tag may represent any of the forms used in English as seen in Figure \ref{fig:leaf_nodes}. In each of these cases, the node inherits the \texttt{verb(\underline{lemma},\underline{tense})} and \texttt{object(\underline{noun},\underline{det},\underline{adj\_or\_adv})} from its children.

For instance, the \textit{verb phrase} ``drank tea" can be captured with the following derivation:

\begin{displayquote}
\begin{lstlisting}
vp -> vbd np {
  verb(N,T) :- verb(N,T)@1.
  object(N,D,A) :- object(N,D,A)@2.
}
\end{lstlisting}
\end{displayquote}

In order to handle continuous tenses, we introduce the predicate \texttt{comp(\underline{first},\underline{second})}. Without changing the \textit{arity} of our predicate \texttt{verb(\underline{lemma},\underline{tense})}, we can use this to combine two verb \textit{lemmas}, as well as two verb tenses.

For example, the derivation that handles the \textit{verb phrase} ``are eating apples" is the following:

\begin{displayquote}
\begin{lstlisting}
vp -> vbp vbg np {
  verb(comp(N1,N2),comp(T1,gerund)) :- verb(N1,T1)@1, verb(N2,gerund)@2.
  object(N,D,A) :- object(N,D,A)@3.
}
\end{lstlisting}
\end{displayquote}

\subsubsection{Sentences}

In order to join sentences (\texttt{s -> np vp}) together we use what is called an \texttt{s\_group}. Defined recursively, these can either be empty or contain another \texttt{s\_group} followed by a sentence (\texttt{s}) and a full-stop:

\begin{displayquote}
\begin{lstlisting}
s_group -> { count(0). }
s_group -> s_group s ``. " { count(X+1) :- count(X)@1. }
\end{lstlisting}
\end{displayquote}

In the way we currently use this general grammar, only a single sentence is allowed per \textit{parse tree} for efficiency reasons, and because it is not necessary. However, if we we were to increase this limit for another application, it could easily be done by changing the first constraint at the root node:

\begin{displayquote}
\begin{lstlisting}
start -> s_group {
   :- count(X)@1, X > 1.
   :- count(X)@1, X = 0.
}
\end{lstlisting}
\end{displayquote}

\section{Learning Actions}
\label{sec:learn_actions}

We first need to convert the pre-processed story's sentences from English into our internal structure. In other words, we need to learn about the \textit{actions} described by the sentences in our story, which can be thought of as high-level semantic descriptors.

\subsection{Formalisation}

We formalise the task of learning an action as \textsc{SumASG\textsubscript{1}(\underline{CFG},\underline{BK},\underline{E})}. Given our general grammar (\textsc{CFG}), a set of context-specific leaf nodes (\textsc{BK}), and a grammar-conforming sentence (\textsc{E}), its goal is to return the \textit{action} corresponding to this sentence, which should have the format \texttt{action(\underline{verb},\underline{subject},\underline{object})}.

However this is not a learning task in the true sense, as we are only interested in generating \textit{ground facts}. It is more of an \textit{abduction} task, whereby we only learn \textit{heads} of \textit{production rules}. In our case, we use this as a mechanism to translate from English into our internal representation.

\subsection{Implementation}

\subsubsection{Positive Example}

In practice, what we do is append to the program containing our general grammar the context-specific leaf nodes (given to us by the \textsc{Preprocessor}), as well as a positive example containing our sentence to learn from. For instance, we would add the positive example for the sentence ``they drove a race-car fast.":

\begin{displayquote}
\begin{lstlisting}[numbers=none]
+ [``they ", ``drove ", ``a ", ``race-car ", ``fast ", ``. "]
\end{lstlisting}
\end{displayquote}

We also need to ensure that the derivation for sentences contains a constraint enforcing that an \texttt{action} be learned when an example is given:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
s -> np vp {
  :- not action(verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), verb(V_N,V_T)@2, subject(S_N,S_D,S_A)@1, object(O_N,O_D,O_A)@2.
  ...
}
\end{lstlisting}
\end{displayquote}

\subsubsection{Mode Bias}

In order guide the learning task, we must also specify a \textit{mode bias} as part of the program for \textsc{SumASG\textsubscript{1}}, which essentially tells ASG the format of the rules which can be learned.

Since we are only interested in learning \textit{facts} (rules with an empty \textit{body)}, it is enough to provide \textit{mode bias} rules of the following form (where \texttt{[4]} restricts the learning task to the fourth derivation):

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#modeh(action(verb(_,_), subject(_,_,_), object(_,_,_)):[4].
\end{lstlisting}
\end{displayquote}

For the most basic of sentences (ones where there is no need to use any \texttt{conjunct} or \texttt{comp} predicates), we use this specific rule:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#modeh(action(verb(const(main_verb),const(main_form)), subject(const(noun),const(det),const(adj_or_adv)), object(const(noun),const(det),const(adj_or_adv)))):[4].
\end{lstlisting}
\end{displayquote}

Such rules require defining ILASP \textit{constants} corresponding to possible \textit{tokens}. To this end, we do so for each word in the \textit{simplified} text.

For the sentence ``they drove a race-car fast.", these would look like this:

\begin{displayquote}
\begin{lstlisting}
#constant(noun,they).
#constant(main_verb,drive).
#constant(main_form,past).
#constant(det,a).
#constant(noun,race_car).
#constant(adj_or_adv,fast).
\end{lstlisting}
\end{displayquote}

\subsubsection{Running}

Once we have augmented our general grammar with all of this information, it is now possible to run the resulting program with the below command, causing an \textit{action} to be output to the command line. Here we use a \textit{depth} of 7, having found that this is the minimum number necessary to ensure that ASG has access to the lowest possible leaf nodes in the tree.

\begin{displayquote}
\begin{lstlisting}[numbers=none, escapechar=\%]
asg action.asg --mode=%\underline{learn}% --depth=7
\end{lstlisting}
\end{displayquote}

For the input sentence ``they drove a race-car fast.", the engine returns a new ASG program without the \textit{mode bias}, where the following \textit{action} has been added to the derivation for sentences (\texttt{s -> np vp}):

\begin{displayquote}
\begin{lstlisting}[numbers=none]
action(verb(drive, past), subject(they, 0, 0), object(race_car, a, fast)).
\end{lstlisting}
\end{displayquote}

\subsection{Search Space Reduction}
\label{subsec:search_space}

The set of rules that a task in ILASP is able to learn, as defined by the \textit{mode bias}, is called the \textit{search space}. The more complex the structure of the rules we can learn, the more of these the engine can generate, and so the larger the \textit{search space}. The more leaf nodes we add, the more combinations of \textit{lemmas} we can create, thereby exponentially growing the \textit{search space}. Since ASG tries to run the program with every single rule in the \textit{search space}, we need to keep this as small as possible.

Throughout the development of \textsc{SumASG\textsubscript{1}}, it was very helpful to view the \textit{search space} via the following command:

\begin{displayquote}
\begin{lstlisting}[numbers=none, escapechar=\%]
asg action.asg --mode=%\underline{ss}% --depth=7
\end{lstlisting}
\end{displayquote}

\subsubsection{Learning Actions Individually}

With this in mind, it is preferable to feed in each sentence separately to \textsc{SumASG\textsubscript{1}}. Although it might seem easier at first to learn them all in one go, doing so individually limits the number of leaf nodes we need to add to the program.

Using this optimization, learning the \textit{actions} from the \textit{simplified} and \textit{homogenized} story of Peter Little takes just a few minutes, rather than many hours.

\subsubsection{Cutting Out Rules}

We have also created a number of \textit{mode bias} rules which eliminate impossible or extremely improbable sentences. With this optimization, we have been able to take the search space size for a simple sentence down from 396 to 16, and from 9477 to 1044 for a more complicated one (i.e., one with more leaf nodes).

For example, the following rule says that we cannot have an \textit{action} where the object of sentence is a conjunction of two words which both have the same \textit{lemma}.

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#bias(":- head(holds_at_node(action(verb(_,_),subject(_,_,_),object(conjunct(V,V),_,_)),var__(1))).").
\end{lstlisting}
\end{displayquote}

Additionally, a number of extraneous rules can appear in the \textit{search space} when we allow for continuous verbs. Continuous verbs are made up of a \textit{main verb} and an \textit{auxiliary verb}. What can happen is that the \textit{search space} contains rules where the \textit{main verb} is never used as such in English (normally always the verb ``to be").

To get around this issue, we enforce that all potential \textit{main verbs} already appear in this form in the input story sentence; the same can be said regarding \textit{auxiliary verbs}. Practically, this means adding \textit{constants} to the program for each \textit{main verb} and \textit{auxiliary verb} appearing in the input.

For instance, the phrase ``are eating" would require the following \textit{constants}:

\begin{displayquote}
\begin{lstlisting}
#constant(main_verb,be).
#constant(aux_verb,eat).
#constant(main_form,present).
#constant(aux_form,gerund).
\end{lstlisting}
\end{displayquote}

Without the optimization, we would end up with a \textit{search space} size of 176 for the sentence ``they are eating apples". We are able to reduce this number to 20 thanks to a \textit{mode bias} that enforces learned continuous verbs to be exactly as they appear in the \textit{simplified} and \textit{homogenized} story:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#modeh(action(verb(comp(const(main_verb),const(aux_verb)),comp(const(main_form),const(aux_form))), subject(const(noun),const(det),const(adj_or_adv)), object(const(noun),const(det),const(adj_or_adv)))):[4].
\end{lstlisting}
\end{displayquote} 

Another way to solve this would have been to add a \textit{mode bias} constraint ruling out cases where both verbs in a continuous form are the same. However, we would usually end up with a \textit{search space} at least as large, since any verb could appear in continuous form. Also, we would have to handle the edge case where both verbs are ``to be", as ``is being" is a perfectly acceptable phrase in English.

\section{Generating Summary Sentences}
\label{sec:gen_summary_sentences}

\subsection{Formalisation}

We formalise the task of generating a \textit{summary sentence} as \textsc{SumASG\textsubscript{2}(\underline{CFG},\underline{BK},\underline{E})}. Given our general grammar (\textsc{CFG}), a set of context-specific leaf nodes for the original story (\textsc{BK}), and a set of learned \textit{actions}, (\textsc{E}), its goal is to return a set of user-readable sentences which may be used to summarize the text.

\subsection{Implementation}

\subsubsection{Learned Actions}

In order to keep the story's chronological ordering, we assign indices to the learned \textit{actions}, inserting this information directly into the \texttt{action} predicates as an additional first argument.

We then put all of these augmented \textit{actions} as rules inside the derivation for sentences (\texttt{s -> np vp}) in our general grammar. 

\subsubsection{Summary Generation Rules And Constraints}

A \textit{summary sentence} should have the same structure as a sentence from the story, so we can define the predicate \texttt{summary} in the same way as we did for \textit{actions}.

Moreover, we create rules whose head is a \texttt{summary} predicate, and whose body contains one or more \texttt{action} predicates. We also assign an identifier to each of these rules, in order to keep track of which one has been used.

In the base case, a \textit{summary sentence} is simply a word-for-word copy of an \textit{action}, in which case we do not care about its position in the story:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
summary(0, V, S, O) :- action(_, V, S, O).
\end{lstlisting}
\end{displayquote}

We create much more complex rules, allowing us to combine information from two \textit{actions} into a single \textit{summary sentence}. In the case where we have two \textit{actions} that share a common \textit{subject} and \textit{verb}, we define a rule that combines these into a single \textit{summary sentence}, preserving the order in which these \textit{objects} appear originally:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
summary(7, V, S, object(conjunct(N1,N2),D,0)) :- action(I1, V, S, object(N1,D,_)), action(I2, V, S, object(N2,D,_)), N1 != N2, N1 != 0, N2 != 0, I1 < I2.
\end{lstlisting}
\end{displayquote}

After having defined a suite of such summarization rules, we now need to apply them using our general grammar. To this end, we add to the derivation for sentences (\texttt{s -> np vp}) a \textit{choice rule}, enforcing with the predicate \texttt{output} that the program must output every derivable \textit{summary sentence} exactly once. Using constraints, an \texttt{s} node can require that its children contain the required information.

\begin{displayquote}
\begin{lstlisting}
0{output(I,V,S,O)}1 :- summary(I,V,S,O).
:- not output(_,_,_,_).

:- output(_,verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), not verb(V_N,V_T)@2.
:- output(_,verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), not subject(S_N,S_D,S_A)@1.
:- output(_,verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), not object(O_N,O_D,O_A)@2.
\end{lstlisting}
\end{displayquote}

\subsubsection{Running}

Once we have augmented the fourth rule of our general grammar (\texttt{s -> np vp}) with the learned \textit{actions} and our set of summary generation rules and constraints, we now generate all the possible \textit{summary sentences} with the below command:

\begin{displayquote}
\begin{lstlisting}[numbers=none, escapechar=\%]
asg summary.asg --mode=%\underline{run}% --depth=7
\end{lstlisting}
\end{displayquote}

\section{Example}

Figure \ref{fig:sumasg_example} shows the different steps of running \textsc{SumASG} on the \textit{simplified} and \textit{homogenized} story of Peter Little, along with a breakdown of the runtime.

After passing in each sentence individually to \textsc{SumASG\textsubscript{1}}, we end up with a list of \textit{actions}, which is essentially the original story translated into our internal representation.

From these \textit{actions} we apply \textsc{SumASG\textsubscript{2}}, generating all possible \textit{summary sentences} which are then used to summarize Peter Little's story.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
\begin{lstlisting}[language=]
action(0, verb(be, past), subject(there, 0, 0), object(boy, a, conjunct(curious, little))).
action(1, verb(comp(be, name), comp(past, past_part)), subject(boy, the, conjunct(curious, little)), object(peterlittle, 0, 0)).
action(2, verb(be, past), subject(peterlittle, 0, 0), object(astronomy, in, curious)).
action(3, verb(be, past), subject(peterlittle, 0, 0), object(school, in, serious)).
action(4, verb(do, past), subject(peterlittle, 0, 0), object(school, 0, always)).
action(5, verb(be, present_third), subject(peterlittle, 0, 0), object(0, 0, conjunct(famous, now))).
\end{lstlisting}
\end{displayquote}
\caption{Results from \textsc{SumASG\textsubscript{1}}}
\end{subfigure}
\begin{subfigure}{\textwidth}
\vspace{\baselineskip}
\begin{displayquote}
\circled{0} PeterLittle was serious in school .\\
\circled{0} PeterLittle was curious in astronomy .\\
\circled{4} PeterLittle was curious and serious .\\
\circled{0} PeterLittle did school always .\\
\circled{0} there was a curious little boy .\\
\circled{0} the curious little boy was named PeterLittle .\\
\circled{0} PeterLittle is famous now .
\end{displayquote}
\caption{Results from \textsc{SumASG\textsubscript{2}} (where the numbers indicate a summary generation rule)}
\vspace{\baselineskip}
\end{subfigure}
\setcounter{subfigure}{0}
\begin{subfigure}{\textwidth}
\begin{subfigure}{0.6\textwidth}
\renewcommand\thesubfigure{\roman{subfigure}}
\centering
\begin{tabular}{@{}lllllll@{}}
\toprule
\textit{Action}         & 0  & 1  & 2 & 3 & 4 & 5 \\ \midrule
Running time (s) & 18 & 32 & 9 & 9 & 7 & 9 \\ \bottomrule
\end{tabular}
\caption{\textsc{SumASG\textsubscript{1}}}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\renewcommand\thesubfigure{\roman{subfigure}}
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Running time (s) & 20 \\ \bottomrule
\end{tabular}
\caption{\textsc{SumASG\textsubscript{2}}}
\end{subfigure}
\setcounter{subfigure}{2}
\caption{Runtime for each step}
\end{subfigure}
\caption{Example of running \textsc{SumASG} for the story of Peter Little}
\label{fig:sumasg_example}
\end{figure}