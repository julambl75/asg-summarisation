\label{chapter:asg}

In this chapter we present the core module of our pipeline: \textsc{SumASG}. After giving an overview of the module (Section \ref{sec:asg_overview}), we introduce a general grammar which we have created specifically for the purpose of representing English sentences in ASG (Section \ref{sec:internal_representation}). We then go further into the details of \textsc{SumASG}'s two-part implementation (Sections \ref{sec:learn_actions} and \ref{sec:gen_summary_sentences}), and finish by showing the results of running \textsc{SumASG} on a pre-processed story (Section \ref{sec:asg_example}).

\section{Overview}
\label{sec:asg_overview}

Our use of ASG is two-fold. Firstly, we pass in each sentence from the story to ASG to obtain its semantic representation in ASP. Secondly, we take these \textit{actions} and use ASG rules to generate possible summary components. These will later be post-processed and turned into actual valid summaries. A diagram of the two ASG steps is shown below in Figure \ref{fig:asg_pipeline}.

{
\floatstyle{plain}
\restylefloat{figure}
\begin{figure}[H]
\centering
\begin{tikzpicture}[node distance=0.55cm, auto]
\node (sentence_1) [inter] {Sentence 1};
\node (sentence_2) [inter, below =of sentence_1] {Sentence 2};
\node (sentence_3) [below =of sentence_2] {...};
\node (sentence_4) [inter, below =of sentence_3] {Sentence n};
\node (learn_action_1) [block, right =of sentence_1] {Learn Action};
\node (learn_action_2) [block, right =of sentence_2, below =of learn_action_1] {Learn Action};
\node (learn_action_3) [right =of sentence_3, below =of learn_action_2] {...};
\node (learn_action_4) [block, right =of sentence_4, below =of learn_action_3] {Learn Action};
\node (gen_summaries) [block, right =of learn_action_2] {Generate Summaries};
\node (summary_sentence_1) [inter, above right =of gen_summaries] {Summary Sentence 1};
\node (summary_sentence_2) [inter, right =of gen_summaries, below =of summary_sentence_1] {Summary Sentence 2};
\node (summary_sentence_3) [right =of gen_summaries, below =of summary_sentence_2] {...};
\node (summary_sentence_4) [inter, right =of gen_summaries, below =of summary_sentence_3] {Summary Sentence m};
\draw [->] (sentence_1) -- (learn_action_1);
\draw [->] (sentence_2) -- (learn_action_2);
\draw [->] (sentence_4) -- (learn_action_4);
\draw [->] (learn_action_1) -- (gen_summaries);
\draw [->] (learn_action_2) -- (gen_summaries);
\draw [->] (learn_action_4) -- (gen_summaries);
\draw [->] (gen_summaries) -- (summary_sentence_1);
\draw [->] (gen_summaries) -- (summary_sentence_2);
\draw [->] (gen_summaries) -- (summary_sentence_4);
\end{tikzpicture}
\caption{ASG steps}
\label{fig:asg_pipeline}
\end{figure}
}

\section{Internal Representation}
\label{sec:internal_representation}

In order to model the structure of sentences in English, we have created a CFG that has a similar hierarchy to that of a \textit{constituency parse tree}. The ASG code for this general structure can be seen in Appendix \ref{appendix:asg}. Throughout this description of \textsc{SumASG}, please refer to Chapter \ref{chapter:background} for information on how to interpret an ASG program. Also, a table listing the possible POS tags is available in Appendix \ref{appendix:pos}.

\subsection{Leaf Nodes}

At the bottom end of the CFG, there are leaf nodes that correspond to individual English words. These nodes get added based on the context, that is to say the words appearing in our story.

Each of these nodes has on the left-hand side (LHS) of the \textit{production rule} its POS tag, and on the right-hand side (RHS) a string containing the word itself. In order to conform to the syntax of ASG, we must write the POS tags in lower-case. Also, we include a space at the end of each word's textual representation so that when we run our program the words appear distinct and not all concatenated together.

In ASG every \textit{production rule} also has a set of ASP rules, which in the case of leaf nodes is just a single rule telling us the word's \textit{lemma} and sentence \textit{role}. In the case of verbs, the \textit{lemma} is simply the base form of the verb, so we also need to keep track of its tense.

For example, leaf nodes for the sentence ``they drove a race-car fast." would look like this:

\begin{displayquote}
\begin{lstlisting}
prp -> ``they " { noun(they). }
vbd -> ``drove " { verb(drive,past). }
dt -> ``a " { det(a). }
nn -> ``race-car " { noun(race_car). }
rb -> ``fast " { adj_or_adv(fast). }
\end{lstlisting}
\end{displayquote}

As part of the input to \textsc{SumASG}, we receive some leaf nodes corresponding to words in the story, where the \textit{lemmas} and \textit{roles} have been assigned by the \textsc{Preprocessor}.

In Figure \ref{fig:leaf_nodes}, you can see which POS tags fall under which \textit{roles}, keeping in mind that this categorisation is only an optimisation and was not intended to strictly adhere to English grammar.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textit{Role}         & POS tags                    \\ \midrule
\texttt{verb(\underline{lemma},\underline{tense})}         & VB, VBD, VBG, VBN, VBP, VBZ \\
\texttt{noun(\underline{lemma})}         & EX, NN, NNS, NNP, NNPS, PRP \\
\texttt{det(\underline{lemma})}          & CD, DT, IN                  \\
\texttt{adj\_or\_adv(\underline{lemma})} & JJ, JJR, JJS, RB, RP        \\ \bottomrule
\end{tabular}
\caption{POS tags by \textit{role}}
\vspace{\baselineskip}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\begin{tabular}{@{}lllllll@{}}
\toprule
POS tag  & VB & VBD  & VBG    & VBN        & VBP     & VBZ            \\ \midrule
Verb tense & \texttt{base} & \texttt{past} & \texttt{gerund} & \texttt{past\_part} & \texttt{present} & \texttt{present\_third} \\ \bottomrule
\end{tabular}
\caption{Verb tense by POS tag}
\end{subfigure}
\caption{Predicates used for the leaf nodes in the internal representation}
\label{fig:leaf_nodes}
\end{figure}

\subsection{Non-Leaf Nodes}

The job of the non-leaf nodes is to join leaf nodes together, matching the way we would join words in English to form a sentence.

In our general grammar, sentences (\texttt{s -> np vp}) are made up a \textit{noun part} (\texttt{np}) followed by a \textit{verb part} (\texttt{vp}). While a \textit{noun part} can be made up of leaf nodes, a \textit{verb part} is always a verb followed by a \textit{noun part}.

\subsubsection*{Noun Parts}

In the \textit{production rule} for a \textit{noun part}, we use logic rules whose role it is to encapsulate a sentence \textit{subject} and/or an \textit{object}. This is done in a bottom-up manner, by using information from the child node(s) to populate a predicate at the \textit{noun part} level.

The reason why we differentiate between these two forms is because some \textit{noun parts} in English are only used as subjects (e.g., existential ``there"), while others can only be objects (e.g., the adjective ``green"), and we want to keep the \textit{search space} as small as possible (see Subsection \ref{subsec:search_space}).

The resulting predicates have respective forms \texttt{subject(\underline{noun},\underline{det},\underline{adj\_or\_adv})} and \texttt{object(\underline{noun},\underline{det},\underline{adj\_or\_adv})}. For all these predicates, we use the \textit{ground term} \texttt{0} to denote the absence of a \textit{token}.

For instance, we can capture the \textit{noun phrase} ``a race-car" using the following \textit{production rule}:

\begin{displayquote}
\begin{lstlisting}
np -> dt nn {
  subject(N,D,0) :- det(D)@1, noun(N)@2.
  object(N,D,0) :- det(D)@1, noun(N)@2.
}
\end{lstlisting}
\end{displayquote}

\noindent
To handle the case of more complex \textit{noun phrases}, we have created a special predicate \texttt{conjunct(\underline{first},\underline{second})}, allowing us to join two words with the same \textit{role}. We also need to add constraints that rule out cases where the two \textit{conjuncts} have the same \textit{lemma}.

For example, the \textit{noun phrase} ``bread and cheese" would be encompassed by the below \textit{production rule}:

\begin{displayquote}
\begin{lstlisting}
np -> nn ``and " nn {
  subject(conjunct(N1,N2),0,0) :- noun(N1)@1, noun(N2)@3.
  object(conjunct(N1,N2),0,0) :- noun(N1)@1, noun(N2)@3.
  :- subject(conjunct(N,N),0,0).
  :- object(conjunct(N,N),0,0).
}
\end{lstlisting}
\end{displayquote}

\subsubsection*{Verb Parts}

The last child node of a \textit{verb part} is always a single \textit{noun part}. Before that comes a verb, whose POS tag may represent any of the forms used in English as seen in Figure \ref{fig:leaf_nodes}. In each of these cases, the node inherits the \texttt{verb(\underline{lemma},\underline{tense})} and \texttt{object(\underline{noun},\underline{det},\underline{adj\_or\_adv})} from its children.

For instance, the \textit{verb phrase} ``drank tea" can be captured with the following \textit{production rule}:

\begin{displayquote}
\begin{lstlisting}
vp -> vbd np {
  verb(N,T) :- verb(N,T)@1.
  object(N,D,A) :- object(N,D,A)@2.
}
\end{lstlisting}
\end{displayquote}

\noindent
In order to handle continuous tenses, we introduce the predicate \texttt{comp(\underline{first},\underline{second})}. Without changing the \textit{arity} of our predicate \texttt{verb(\underline{lemma},\underline{tense})}, we can use this to combine two verb \textit{lemmas}, as well as two verb tenses.

For example, the \textit{production rule} that handles the \textit{verb phrase} ``are eating apples" is the following:

\begin{displayquote}
\begin{lstlisting}
vp -> vbp vbg np {
  verb(comp(N1,N2),comp(T1,gerund)) :- verb(N1,T1)@1, verb(N2,gerund)@2.
  object(N,D,A) :- object(N,D,A)@3.
}
\end{lstlisting}
\end{displayquote}

\subsubsection*{Sentences}

In order to join sentences (\texttt{s -> np vp}) together we use what is called an \texttt{s\_group}. Defined recursively, these can either be empty or contain another \texttt{s\_group} followed by a sentence (\texttt{s}) and a full-stop:

\begin{displayquote}
\begin{lstlisting}
s_group -> { count(0). }
s_group -> s_group s ``. " { count(X+1) :- count(X)@1. }
\end{lstlisting}
\end{displayquote}

\noindent
In the way we currently use this general grammar, only a single sentence is allowed per \textit{parse tree} for efficiency reasons. However, if we we were to increase this limit for another application, it could easily be done by changing the first constraint at the root node (line 2 in the code below):

\begin{displayquote}
\begin{lstlisting}
start -> s_group {
   :- count(X)@1, X > 1.
   :- count(X)@1, X = 0.
}
\end{lstlisting}
\end{displayquote}

\section{Learning Actions}
\label{sec:learn_actions}

We first convert the pre-processed story's sentences from English into our internal ASG structure. In other words, we learn about the \textit{actions} described by the sentences in our story, which can be thought of as high-level semantic descriptors.

\subsection{Formalisation}

We formalise the task of learning an action as \textsc{SumASG\textsubscript{1}(\underline{CFG},\underline{BK},\underline{E})}. Given our general grammar (\textsc{CFG}), a set of context-specific leaf nodes (\textsc{BK}), and a grammar-conforming sentence (\textsc{E}), its goal is to return the \textit{action} corresponding to this sentence, which should have the format \texttt{action(\underline{verb},\underline{subject},\underline{object})}.

However this is not a learning task in the true sense, as we are only interested in generating \textit{ground facts}. It is more of an \textit{abduction} task, whereby we only learn \textit{heads} of \textit{production rules}. In our case, we use this as a mechanism to translate from English into our internal representation.

\subsection{Implementation}

In practice, this translation involves taking our general grammar and, on a per-sentence basis (see Subsection \ref{subsec:search_space}), appending to it the sentence's context-specific leaf nodes (given to us by the \textsc{Preprocessor}), a \textit{positive example} (containing the sentence itself), as well as a \textit{mode bias} for learning \textit{actions}.

\subsubsection*{Positive Example}

To give a \textit{positive example} to \textsc{SumASG\textsubscript{1}}, we must provide the sentence as a list of \textit{tokens}. For instance, we would use the following \textit{positive example} for the sentence ``they drove a race-car fast.":

\begin{displayquote}
\begin{lstlisting}[numbers=none]
+ [``they ", ``drove ", ``a ", ``race-car ", ``fast ", ``. "]
\end{lstlisting}
\end{displayquote}

\noindent
In order to ensure that this \textit{positive example} contributes to learning a corresponding \textit{action}, we also need to add a constraint to the \textit{production rule} for sentences (\texttt{s -> np vp}). Intuitively, the rule shown in line 2 below says that if we have a sentence (i.e., our \textit{positive example}) which consists of a given \textit{verb}, \textit{subject} and \textit{object}, then we need to learn the matching \textit{action}.

\begin{displayquote}
\begin{lstlisting}
s -> np vp {
  :- not action(verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), verb(V_N,V_T)@2, subject(S_N,S_D,S_A)@1, object(O_N,O_D,O_A)@2.
  ...
}
\end{lstlisting}
\end{displayquote}

\subsubsection*{Mode Bias}

In order to guide the learning task, we must also specify a \textit{mode bias} as part of the program for \textsc{SumASG\textsubscript{1}}, which essentially tells ASG the format of the rules which can be learned.

Since we are only interested in learning \textit{facts} (rules with an empty \textit{body)}, it is enough to provide \textit{mode bias} rules of the following form (where \texttt{[4]} restricts the learning task to the fourth \textit{production rule}):

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#modeh(action(verb(_,_), subject(_,_,_), object(_,_,_)):[4].
\end{lstlisting}
\end{displayquote}

\noindent
For the most basic of sentences (ones where there is no need to use any \texttt{conjunct} or \texttt{comp} predicates), we use this specific rule:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#modeh(action(verb(const(main_verb),const(main_form)), subject(const(noun),const(det),const(adj_or_adv)), object(const(noun),const(det),const(adj_or_adv)))):[4].
\end{lstlisting}
\end{displayquote}

\noindent
Such rules require defining ILASP \textit{constants} corresponding to possible \textit{tokens}. To this end, we do so for each word in the \textit{simplified} text. For the sentence ``they drove a race-car fast.", these would look like this:

\begin{displayquote}
\begin{lstlisting}
#constant(noun,they).
#constant(main_verb,drive).
#constant(main_form,past).
#constant(det,a).
#constant(noun,race_car).
#constant(adj_or_adv,fast).
\end{lstlisting}
\end{displayquote}

\noindent
After running the full learning task on this example, the ASG engine returns a new program where the following \textit{action} has been added to the \textit{production rule} for sentences (\texttt{s -> np vp}):

\begin{displayquote}
\begin{lstlisting}[numbers=none]
action(verb(drive, past), subject(they, 0, 0), object(race_car, a, fast)).
\end{lstlisting}
\end{displayquote}

\noindent
After doing this for each sentence in the pre-processed story, we end up with at most as many \textit{actions} as there are sentences in this text (those which do not \textit{conform} to our general grammar are ignored).

\subsection{Search Space Reduction}
\label{subsec:search_space}

The set of rules that a task in ILASP is able to learn, as defined by the \textit{mode bias}, is called the \textit{search space}. The more complex the structure of the rules we can learn, the more of these the engine can generate, and so the larger the \textit{search space}. The more leaf nodes we add, the more combinations of \textit{lemmas} we can create, thereby exponentially growing the \textit{search space}. Since ASG tries to run the program with every single rule in the \textit{search space}, we need to keep this as small as possible.

\subsubsection*{Learning Actions Individually}

With this in mind, it is preferable to feed in each sentence separately to \textsc{SumASG\textsubscript{1}}. Although it might seem easier at first to learn them all in one go, doing so individually limits the number of leaf nodes we need to add to the program.

Using this optimisation, learning the \textit{actions} from the \textit{simplified} and \textit{homogenized} story of Peter Little takes just a few minutes, rather than many hours.

\subsubsection*{Cutting Out Rules}

We have also created a number of \textit{mode bias} rules which eliminate impossible or extremely improbable sentences. With this optimisation, we have been able to take the search space size for a simple sentence down from 396 to 16, and from 9477 to 1044 for a more complicated one (i.e., one with more leaf nodes).

For example, the following rule says that we cannot have an \textit{action} where the object of sentence is a conjunction of two words which both have the same \textit{lemma}.

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#bias(":- head(holds_at_node(action(verb(_,_),subject(_,_,_),object(conjunct(V,V),_,_)),var__(1))).").
\end{lstlisting}
\end{displayquote}

\noindent
Additionally, a number of extraneous rules can appear in the \textit{search space} when we allow for continuous verbs. Continuous verbs are made up of a \textit{main verb} and an \textit{auxiliary verb}. What can happen is that the \textit{search space} contains rules where the \textit{main verb} is never used as such in English (normally always the verb ``to be").

To get around this issue, we enforce that all potential \textit{main verbs} already appear in this form in the input story sentence; the same can be said regarding \textit{auxiliary verbs}. Practically, this means adding \textit{constants} to the program for each \textit{main verb} and \textit{auxiliary verb} appearing in the input.

For instance, the phrase ``are eating" would require the following \textit{constants}:

\begin{displayquote}
\begin{lstlisting}
#constant(main_verb,be).
#constant(aux_verb,eat).
#constant(main_form,present).
#constant(aux_form,gerund).
\end{lstlisting}
\end{displayquote}

\noindent
Without the optimisation, we would end up with a \textit{search space} size of 176 for the sentence ``they are eating apples". We are able to reduce this number to 20 thanks to a \textit{mode bias} that enforces learned continuous verbs to be exactly as they appear in the \textit{simplified} and \textit{homogenized} story:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
#modeh(action(verb(comp(const(main_verb),const(aux_verb)),comp(const(main_form),const(aux_form))), subject(const(noun),const(det),const(adj_or_adv)), object(const(noun),const(det),const(adj_or_adv)))):[4].
\end{lstlisting}
\end{displayquote} 

\noindent
Another way to solve this would have been to add a \textit{mode bias} constraint ruling out cases where both verbs in a continuous form are the same. However, we would usually end up with a \textit{search space} at least as large, since any verb could appear in continuous form. Also, we would have to handle the edge case where both verbs are ``to be", as ``is being" is a perfectly acceptable phrase in English.

\section{Generating Summary Sentences}
\label{sec:gen_summary_sentences}

The second part of \textsc{SumASG} deals with generating summary sentences using the \textit{actions} that were learned from the story in the previous step.

\subsection{Formalisation}

We formalise the task of generating a \textit{summary sentence} as \textsc{SumASG\textsubscript{2}(\underline{CFG},\underline{BK},\underline{E})}. Given our general grammar (\textsc{CFG}), a set of context-specific leaf nodes for the original story (\textsc{BK}), and a set of learned \textit{actions}, (\textsc{E}), its goal is to return a set of English sentences which may be used to summarize the text.

\subsection{Implementation}

In practice, this task involves gathering all of the story-specific leaf nodes and learned \textit{actions} from \textsc{SumASG\textsubscript{1}}, adding these to our general grammar, and then using a set of summary generation rules to create \textit{summary sentences}.

\subsubsection*{Learned Actions}

In order to keep the story's chronological ordering, we assign indices to the learned \textit{actions}, inserting this information directly into the \texttt{action} predicates as an additional first argument.

We then put all of these augmented \textit{actions} as rules inside the \textit{production rule} for sentences (\texttt{s -> np vp}) in our general grammar. 

\subsubsection*{Summary Generation Rules And Constraints}

A \textit{summary sentence} should have the same structure as a sentence from the story, so we can define the predicate \texttt{summary} in the same way as we did for \textit{actions}.

Moreover, we create rules whose head is a \texttt{summary} predicate, and whose body contains one or more \texttt{action} predicates. We also assign an identifier to each of these rules, in order to keep track of which one has been used.

In the base case, a \textit{summary sentence} is simply a word-for-word copy of an \textit{action}, in which case we do not care about its position in the story:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
summary(0, V, S, O) :- action(_, V, S, O).
\end{lstlisting}
\end{displayquote}

\noindent
We also have more complex rules, allowing us to combine information from multiple \textit{actions} into a single \textit{summary sentence}. In the case where we have two \textit{actions} that share a common \textit{subject} and \textit{verb}, we define a rule that combines these into a single \textit{summary sentence}, preserving the order in which these \textit{objects} appear originally:

\begin{displayquote}
\begin{lstlisting}[numbers=none]
summary(9, V, S, object(conjunct(N1,N2),D,0)) :- action(I1, V, S, object(N1,D,_)), action(I2, V, S, object(N2,D,_)), N1 != N2, N1 != 0, N2 != 0, I1 < I2.
\end{lstlisting}
\end{displayquote}

\noindent
After having defined a suite of such summarization rules, we now need to apply them using our general grammar. To this end, we add to the \textit{production rule} for sentences (\texttt{s -> np vp}) a \textit{choice rule}, enforcing with the predicate \texttt{output} that the program must output every derivable \textit{summary sentence} exactly once. Using constraints, we say that for each \texttt{output} (\textit{summary sentence}), the child nodes of a sentence (\texttt{s}) must contain the \textit{verb}, \textit{subject} and \textit{object} corresponding to the given \texttt{ouput}:

\begin{displayquote}
\begin{lstlisting}
0{output(I,V,S,O)}1 :- summary(I,V,S,O).
:- not output(_,_,_,_).

:- output(_,verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), not verb(V_N,V_T)@2.
:- output(_,verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), not subject(S_N,S_D,S_A)@1.
:- output(_,verb(V_N,V_T),subject(S_N,S_D,S_A),object(O_N,O_D,O_A)), not object(O_N,O_D,O_A)@2.
\end{lstlisting}
\end{displayquote}

\noindent
Once we have augmented the \textit{production rule} for sentences in our general grammar (\texttt{s -> np vp}) with the learned \textit{actions} and our set of summary generation rules and constraints, we use to ASG engine to output all strings \textit{conforming} to this augmented grammar. These strings correspond exactly to the possible \textit{summary sentences} as written in English, which is what we use as the output for \textsc{SumASG\textsubscript{2}}.

\section{Example}
\label{sec:asg_example}

Now that we have discussed how \textsc{SumASG} works, we can show the results of running it on the pre-processed story of Peter Little. Figure \ref{fig:sumasg_example} shows the respective outputs of \textsc{SumASG\textsubscript{1}} and \textsc{SumASG\textsubscript{2}}, as well as a breakdown of the runtime.

After passing in each sentence individually to \textsc{SumASG\textsubscript{1}}, we end up with a list of \textit{actions}, which is essentially the original story translated into our internal representation.

From these \textit{actions} we apply \textsc{SumASG\textsubscript{2}}, generating all possible \textit{summary sentences} which are then used to summarize Peter Little's story.

\begin{figure}[H]
\begin{subfigure}{\textwidth}
\begin{displayquote}
\begin{lstlisting}[language=]
action(0, verb(be, past), subject(there, 0, 0), object(boy, a, conjunct(curious, little))).
action(1, verb(comp(be, name), comp(past, past_part)), subject(boy, the, conjunct(curious, little)), object(peterlittle, 0, 0)).
action(2, verb(be, past), subject(peterlittle, 0, 0), object(astronomy, in, curious)).
action(3, verb(be, past), subject(peterlittle, 0, 0), object(school, in, serious)).
action(4, verb(do, past), subject(peterlittle, 0, 0), object(school, 0, always)).
action(5, verb(be, present_third), subject(peterlittle, 0, 0), object(0, 0, conjunct(famous, now))).
\end{lstlisting}
\end{displayquote}
\caption{Results from \textsc{SumASG\textsubscript{1}}}
\end{subfigure}
\begin{subfigure}{\textwidth}
\vspace{\baselineskip}
\begin{displayquote}
\circled{0} PeterLittle was serious in school .\\
\circled{0} PeterLittle was curious in astronomy .\\
\circled{4} PeterLittle was curious and serious .\\
\circled{0} PeterLittle did school always .\\
\circled{0} there was a curious little boy .\\
\circled{0} the curious little boy was named PeterLittle .\\
\circled{0} PeterLittle is famous now .
\end{displayquote}
\caption{Results from \textsc{SumASG\textsubscript{2}} (where the numbers indicate a summary generation rule)}
\vspace{\baselineskip}
\end{subfigure}
\setcounter{subfigure}{0}
\begin{subfigure}{\textwidth}
\begin{subfigure}{0.6\textwidth}
\renewcommand\thesubfigure{\roman{subfigure}}
\centering
\begin{tabular}{@{}lllllll@{}}
\toprule
\textit{Action}         & 0  & 1  & 2 & 3 & 4 & 5 \\ \midrule
Running time (s) & 18 & 32 & 9 & 9 & 7 & 9 \\ \bottomrule
\end{tabular}
\caption{\textsc{SumASG\textsubscript{1}}}
\end{subfigure}
\begin{subfigure}{0.4\textwidth}
\renewcommand\thesubfigure{\roman{subfigure}}
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Running time (s) & 20 \\ \bottomrule
\end{tabular}
\caption{\textsc{SumASG\textsubscript{2}}}
\end{subfigure}
\setcounter{subfigure}{2}
\caption{Runtime for each step}
\end{subfigure}
\caption{Example of running \textsc{SumASG} for the story of Peter Little}
\label{fig:sumasg_example}
\end{figure}